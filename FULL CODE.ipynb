{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "dXUPxCCeL8AP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Data preprocessing\n",
        "first iomt traffic dataset link: https://zenodo.org/records/8116338\n",
        "second dataset ice dataset link: http://perception.inf.um.es/ICE-datasets/\n",
        "third wustl dataset link: https://www.cse.wustl.edu/~jain/ehms/index.html"
      ],
      "metadata": {
        "id": "dXUPxCCeL8AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Iomt traffic dataset into 2 datasets to more client simulation without reordering anything**"
      ],
      "metadata": {
        "id": "xQzvkAsZp5g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === CONFIG ===\n",
        "INPUT_CSV  = \"/content/drive/MyDrive/fed_MID/data/iomt_trafficdata/IP-Based-Flows-Dataset.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/fed_MID/iomt_clients\"\n",
        "CLIENT_A   = \"client_iomt_A.csv\"\n",
        "CLIENT_B   = \"client_iomt_B.csv\"\n",
        "# ==============\n",
        "\n",
        "import os, pathlib, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pathlib.Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print(\"Reading:\", INPUT_CSV)\n",
        "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
        "df[\"_orig_idx\"] = np.arange(len(df))  # preserve original order for flow-pure splits\n",
        "\n",
        "# --- normalise some columns (keep original names too) ---\n",
        "cols = {c.lower(): c for c in df.columns}\n",
        "traffic_col = cols.get(\"traffic\") or cols.get(\"scenario\") or cols.get(\"label\") or None\n",
        "service_col = cols.get(\"service\")\n",
        "proto_col   = cols.get(\"proto\")\n",
        "\n",
        "def lc_series(s):\n",
        "    return s.astype(str).str.strip().str.lower()\n",
        "\n",
        "t = lc_series(df[traffic_col]) if traffic_col else pd.Series([\"\"], index=df.index)\n",
        "s = lc_series(df[service_col]) if service_col else pd.Series([\"\"], index=df.index)\n",
        "p = lc_series(df[proto_col])   if proto_col   else pd.Series([\"\"], index=df.index)\n",
        "\n",
        "# --- define families by scenario keywords (tuned to this dataset) ---\n",
        "A_kw = (\n",
        "    r\"apache\\s*killer|rudy|slow\\s*loris|slow\\s*read|malaria|mqtt|http\"\n",
        ")\n",
        "B_kw = (\n",
        "    r\"\\barp\\b|arp\\s*spoof|cam(\\s|_|-)?table|net\\s*scan|netscan|dns\"\n",
        ")\n",
        "\n",
        "assign = pd.Series(\"\", index=df.index, dtype=\"object\")\n",
        "assign = np.where(t.str.contains(A_kw, regex=True), \"A\", assign)\n",
        "assign = np.where((assign==\"\") & t.str.contains(B_kw, regex=True), \"B\", assign)\n",
        "\n",
        "# --- fallback by service family (if scenario text was missing) ---\n",
        "assign = np.where((assign==\"\") & s.isin([\"http\",\"mqtt\"]), \"A\", assign)\n",
        "assign = np.where((assign==\"\") & s.isin([\"arp\",\"dns\"]), \"B\", assign)\n",
        "\n",
        "# --- final fallback: stable hash on a 5-tuple (keeps clients disjoint, deterministic) ---\n",
        "def stable_bit(a):\n",
        "    h = hashlib.md5((\"||\".join(map(lambda x: \"\" if pd.isna(x) else str(x), a))).encode(\"utf-8\")).hexdigest()\n",
        "    return int(h, 16) & 1\n",
        "\n",
        "# vectorised hash: apply on a dataframe of fields\n",
        "hash_fields = pd.DataFrame({\n",
        "    \"id.orig_h\": df.get(\"id.orig_h\", \"\"),\n",
        "    \"id.orig_p\": df.get(\"id.orig_p\", \"\"),\n",
        "    \"id.resp_h\": df.get(\"id.resp_h\", \"\"),\n",
        "    \"id.resp_p\": df.get(\"id.resp_p\", \"\"),\n",
        "    \"proto\":     df.get(proto_col, s)  # fallback to service if proto missing\n",
        "})\n",
        "bits = hash_fields.astype(str).agg(\"||\".join, axis=1).apply(lambda x: int(hashlib.md5(x.encode()).hexdigest(), 16) & 1)\n",
        "assign = np.where(assign==\"\", np.where(bits==0, \"A\", \"B\"), assign)\n",
        "\n",
        "df[\"__client__\"] = assign\n",
        "\n",
        "# --- binary attack indicator (robust to various label schemes) ---\n",
        "def infer_binary_attack(frame):\n",
        "    # prefer is_attack if available\n",
        "    for cand in [\"is_attack\",\"Is_Attack\",\"label\",\"Label\",\"class\",\"Class\"]:\n",
        "        if cand in frame.columns:\n",
        "            col = frame[cand]\n",
        "            if col.dtype.kind in \"biufc\":\n",
        "                return (pd.to_numeric(col, errors=\"coerce\").fillna(0) != 0).astype(\"int8\")\n",
        "            s = col.astype(str).str.lower().str.strip()\n",
        "            return (~s.str.contains(\"benign|normal|clean\")).astype(\"int8\")\n",
        "    # fallback to traffic scenario\n",
        "    if traffic_col:\n",
        "        s = lc_series(frame[traffic_col])\n",
        "        return (~s.str.contains(\"benign|normal|clean\")).astype(\"int8\")\n",
        "    # last resort: everything benign\n",
        "    return pd.Series(np.zeros(len(frame), dtype=\"int8\"), index=frame.index)\n",
        "\n",
        "df[\"Label_binary\"] = infer_binary_attack(df)\n",
        "\n",
        "# --- write clients in original file order (flow-pure) ---\n",
        "dfA = df.loc[df[\"__client__\"]==\"A\"].sort_values(\"_orig_idx\").drop(columns=[\"__client__\"])\n",
        "dfB = df.loc[df[\"__client__\"]==\"B\"].sort_values(\"_orig_idx\").drop(columns=[\"__client__\"])\n",
        "\n",
        "outA = os.path.join(OUTPUT_DIR, CLIENT_A)\n",
        "outB = os.path.join(OUTPUT_DIR, CLIENT_B)\n",
        "dfA.to_csv(outA, index=False)\n",
        "dfB.to_csv(outB, index=False)\n",
        "\n",
        "def summarize(name, frame):\n",
        "    pos = int(frame[\"Label_binary\"].sum()) if \"Label_binary\" in frame.columns else 0\n",
        "    n   = len(frame)\n",
        "    pr  = (100.0*pos/n) if n else 0.0\n",
        "    print(f\"{name}: rows={n:,} | attacks={pos:,} ({pr:.2f}%)\")\n",
        "\n",
        "print(\"\\n=== Split summary ===\")\n",
        "summarize(\"Client A (IoMT_A)\", dfA)\n",
        "summarize(\"Client B (IoMT_B)\", dfB)\n",
        "print(\"Saved:\", outA, \"and\", outB)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c_AFsW-2ewS",
        "outputId": "df7a82bc-c906-4759-891e-d642eb9b4a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading: /content/drive/MyDrive/fed_MID/data/iomt_trafficdata/IP-Based-Flows-Dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " converting ice ransomware dataset from binetflow to csv"
      ],
      "metadata": {
        "id": "8Qp30OeTqG9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "ICE_DIR   = \"/content/drive/MyDrive/fed_MID/data/ICEdataset\"\n",
        "OUT_CSV   = \"/content/drive/MyDrive/fed_MID/iomt_clients/client_ICE.csv\"\n",
        "\n",
        "\n",
        "def read_binetflow_with_rowid(path, file_id):\n",
        "    # read as-is; order preserved\n",
        "    df = pd.read_csv(path, sep=\",\", engine=\"python\", comment=\"#\",\n",
        "                     skip_blank_lines=True, on_bad_lines=\"skip\")\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    df[\"scenario\"] = pathlib.Path(path).stem.lower()\n",
        "    df[\"__file_id__\"] = file_id\n",
        "    df[\"__row_in_file__\"] = range(len(df))  # preserves original row order\n",
        "    return df\n",
        "\n",
        "# define a deterministic file order (edit if you prefer a different sequence)\n",
        "files = [\n",
        "    os.path.join(ICE_DIR, \"clean.binetflow\"),\n",
        "    os.path.join(ICE_DIR, \"wannacry.binetflow\"),\n",
        "    os.path.join(ICE_DIR, \"petya.binetflow\"),\n",
        "    os.path.join(ICE_DIR, \"badrabbit.binetflow\"),\n",
        "    os.path.join(ICE_DIR, \"powerghost.binetflow\"),\n",
        "]\n",
        "parts = [read_binetflow_with_rowid(p, i) for i, p in enumerate(files)]\n",
        "df = pd.concat(parts, ignore_index=True)\n",
        "\n",
        "# standardize is_attack without touching order\n",
        "label_col = next((c for c in [\"Label\",\"label\",\"is_attack\",\"IsAttack\"] if c in df.columns), None)\n",
        "if label_col:\n",
        "    s = df[label_col].astype(str).str.strip().str.lower()\n",
        "    df[\"is_attack\"] = (~s.str.contains(\"^normal$\", na=False)).astype(int)\n",
        "else:\n",
        "    df[\"is_attack\"] = (~df[\"scenario\"].str.contains(\"clean\", na=False)).astype(int)\n",
        "\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(\"Saved:\", OUT_CSV)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NrDU_owT3SdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline standardises raw IoMT datasets for federated learning. It:\n",
        "\n",
        "Loads client CSVs and infers binary labels.\n",
        "\n",
        "Cleans features (removes IDs/biometrics, fixes missing/infinite values).\n",
        "\n",
        "Splits into train/val/test with balance checks.\n",
        "\n",
        "Normalises, scales, and windows the data for time-series learning.\n",
        "\n",
        "Saves processed arrays, metadata, and validation reports for consistency."
      ],
      "metadata": {
        "id": "b3hgRMgvOjw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhww8vgBFl4X",
        "outputId": "b11117ef-6d2a-4c9e-f739-8e9b9298c786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 — Config\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Input CSVs\n",
        "ICE_CSV   = \"/content/drive/MyDrive/fed_MID/iomt_clients/client_ICE.csv\"\n",
        "IOMT_B_CSV  = \"/content/drive/MyDrive/fed_MID/iomt_clients/client_iomt_B.csv\"\n",
        "WUSTL_CSV = \"/content/drive/MyDrive/fed_MID/iomt_clients/client_WUSTL.csv\"\n",
        "IOMT_A_CSV  = \"/content/drive/MyDrive/fed_MID/iomt_clients/client_iomt_A.csv\"\n",
        "# Output directory for current dataset (change per run)\n",
        "OUT_ICE     = \"/content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/ICE\"\n",
        "OUT_IOMT_A     = \"/content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/IOMT_A\"\n",
        "OUT_IOMT_B     = \"/content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/IOMT_B\"\n",
        "OUT_WUSTL     = \"/content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/WUSTL\"\n",
        "DATASET_NAME = \"IOMT_B\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os, pathlib, numpy as np, random\n",
        "np.random.seed(42); random.seed(42)\n",
        "\n",
        "# Parameters\n",
        "W = 50\n",
        "HORIZONS = (1,5,10)\n",
        "STRIDE = 1                 # keep 1 to guarantee windows\n",
        "CLIP_Q_LOW, CLIP_Q_HIGH = 0.005, 0.995\n",
        "SAVE_AS_MEMMAP = True      # writes .npy (not .npz)\n",
        "\n",
        "# --- splitting config ---\n",
        "SPLIT_MODE = \"AUTO\"         # \"CONTIG\", \"CHUNKED\", or \"AUTO\"\n",
        "CHUNK_ROWS = 20_000        # size of contiguous chunks for chunked split\n",
        "MIN_CHUNK_ROWS = 5_000\n",
        "MAX_CHUNK_RETRIES = 3\n",
        "POS_EXTREME = 0.98           # if val/test pre-window pos-rate >= 0.98 (or <= 0.02) we retry\n",
        "VAL_FRAC   = 0.05\n",
        "TEST_FRAC  = 0.15\n",
        "# Guards\n",
        "assert all(h > 0 for h in HORIZONS), \"All horizons must be > 0\"\n",
        "assert STRIDE >= 1, \"STRIDE must be >= 1\"\n",
        "pathlib.Path(OUT_ICE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Toggle EDA\n",
        "RUN_EDA = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below defines helper functions for robust CSV reading, automatic inference of binary attack labels from diverse dataset columns, and addition of base metadata (row indices, file name, unique IDs). The label inference logic harmonises heterogeneous sources by mapping different label formats into a consistent Label_binary (0/1)."
      ],
      "metadata": {
        "id": "KBHR953TQrUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell2\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def read_csv_robust(path):\n",
        "    return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
        "\n",
        "def infer_binary_label(df):\n",
        "    \"\"\"Return series Label_binary (0/1) and keep original in Label_raw if present.\"\"\"\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    for name in [\"is_attack\",\"label_binary\",\"label\",\"class\",\"attack\",\"attack category\",\"attack_category\"]:\n",
        "        if name in cols:\n",
        "            raw = df[cols[name]]\n",
        "            if raw.dtype.kind in \"biufc\":\n",
        "                return (raw.fillna(0).astype(float)!=0).astype(\"int8\"), cols[name]\n",
        "            s = raw.astype(str).str.strip().str.lower()\n",
        "            pos = ~(s.str.fullmatch(\"benign|normal|clean\"))\n",
        "            return pos.astype(\"int8\"), cols[name]\n",
        "    if \"scenario\" in df.columns:\n",
        "        pos = ~df[\"scenario\"].astype(str).str.lower().str.contains(\"clean|normal\")\n",
        "        return pos.astype(\"int8\"), \"scenario\"\n",
        "    raise ValueError(\"Could not infer binary label column.\")\n",
        "\n",
        "def add_base_meta(df, src_path):\n",
        "    df = df.copy()\n",
        "    df[\"_orig_idx\"] = np.arange(len(df))       # used for positional split\n",
        "    df[\"Label_raw_file\"] = os.path.basename(src_path)\n",
        "    df[\"row_uid\"] = df[\"Label_raw_file\"].astype(str) + \"#\" + df[\"_orig_idx\"].astype(str)\n",
        "    df[\"Split_hint\"] = \"all\"\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "hJ1sUyyFUlO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data integrity checks: The cell below audits duplicates and missing values without dropping any rows. In sequential traffic data, row order is critical because models learn from sliding windows over consecutive flows. Removing duplicates could disrupt temporal continuity and harm sequence learning. Instead, duplicates are hashed and audited to detect possible leakage across splits, while preserving the full flow order for training."
      ],
      "metadata": {
        "id": "yyrqaSgXR91M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell3\n",
        "def numeric_coerce_cols(df, names):\n",
        "    for n in names:\n",
        "        if n in df.columns:\n",
        "            df[n] = pd.to_numeric(df[n], errors=\"coerce\")\n",
        "\n",
        "def content_hash_row(vals):\n",
        "    # format numbers consistently; join; sha1\n",
        "    s = []\n",
        "    for v in vals:\n",
        "        if isinstance(v, (float, np.floating, int, np.integer)):\n",
        "            try: s.append(f\"{float(v):.10g}\")\n",
        "            except: s.append(str(v))\n",
        "        else:\n",
        "            s.append(str(v))\n",
        "    return hashlib.sha1(\"|\".join(s).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def audit_duplicates_no_drop(df, feature_cols, label_col=\"Label_binary\", audit_path=None):\n",
        "    \"\"\"\n",
        "    Compute a stable content_hash over feature_cols, group stats, and write an audit CSV.\n",
        "    Does NOT drop any rows .\n",
        "    \"\"\"\n",
        "    hashes = df[feature_cols].apply(lambda r: content_hash_row(r.values), axis=1)\n",
        "    df = df.assign(content_hash=hashes)\n",
        "\n",
        "    stats = df.groupby(\"content_hash\")[label_col].agg(\n",
        "        total_rows=\"count\",\n",
        "        nlabels=lambda s: s.nunique(),\n",
        "        pos_count=lambda s: (s.astype(int) == 1).sum()\n",
        "    ).reset_index()\n",
        "    stats[\"neg_count\"] = stats[\"total_rows\"] - stats[\"pos_count\"]\n",
        "\n",
        "    if audit_path is not None:\n",
        "        try:\n",
        "            stats.to_csv(audit_path, index=False)\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] audit write failed: {e}\")\n",
        "\n",
        "    return df, 0, stats\n",
        "\n",
        "\n",
        "def missing_inf_report(df):\n",
        "    rep = {}\n",
        "    for c in df.columns:\n",
        "        s = pd.to_numeric(df[c], errors=\"coerce\") if df[c].dtype == object else df[c]\n",
        "        if np.issubdtype(s.dtype, np.number):\n",
        "            rep[c] = {\n",
        "                \"missing\": int(s.isna().sum()),\n",
        "                \"pos_inf\": int(np.isposinf(s).sum()),\n",
        "                \"neg_inf\": int(np.isneginf(s).sum())\n",
        "            }\n",
        "    return rep\n"
      ],
      "metadata": {
        "id": "-ZJPKzEeFqDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature curation**\n",
        "This cell below is to prevents leakage and reduces noise. Raw ports are bucketed into categories (well-known, registered, ephemeral). Identifiers (IPs, MACs, UIDs), timestamps, and label/meta fields are dropped to avoid shortcuts. Small categorical fields are one-hot encoded, and biometric columns are removed for privacy. Finally, a negative-value audit flags unexpected negatives in non-signed features."
      ],
      "metadata": {
        "id": "mmztTTbGSOHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell4\n",
        "import re\n",
        "\n",
        "\n",
        "def port_bucket_series(s):\n",
        "    p = pd.to_numeric(s, errors=\"coerce\")\n",
        "    cat = pd.Series(\"unknown\", index=s.index)\n",
        "    cat = np.where((p>=0)&(p<=1023), \"well_known\", cat)\n",
        "    cat = np.where((p>=1024)&(p<=49151), \"registered\", cat)\n",
        "    cat = np.where((p>=49152)&(p<=65535), \"ephemeral\", cat)\n",
        "    return pd.Series(cat, index=s.index, dtype=\"category\")\n",
        "\n",
        "def apply_port_flags(df):\n",
        "    df = df.copy()\n",
        "    port_candidates = [\n",
        "     c for c in df.columns\n",
        "     if re.search(r\"(?:^|[_\\.])(sport|dport|id\\.orig_p|id\\.resp_p)$\", c, re.I)\n",
        "    ]\n",
        "    raw_to_drop = []\n",
        "    for c in port_candidates:\n",
        "        b = port_bucket_series(df[c])\n",
        "        for val in [\"well_known\",\"registered\",\"ephemeral\"]:\n",
        "            df[f\"{c}_bucket_{val}\"] = (b==val).astype(\"int8\")\n",
        "        raw_to_drop.append(c)\n",
        "    df.drop(columns=raw_to_drop, inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "# safer ID-like pattern: DO NOT include 'ip' (keeps resp_ip_bytes etc.)\n",
        "ID_LIKE_PAT = re.compile(r\"(mac|addr|address|uuid|uid|guid|session|ssid|hash|md5|sha1|patient|nhs|ssn|room|bed|ward)\", re.I)\n",
        "TIME_LIKE_PAT = re.compile(r\"(time|timestamp|ts|starttime|date)\", re.I)\n",
        "\n",
        "NEVER_FEATURES = set([\"row_uid\",\"content_hash\",\"_orig_idx\",\"Label_raw_file\",\"Split_hint\",\"Label_raw\"])\n",
        "\n",
        "EXPLICIT_BLOCK = set([\n",
        "    # network identifiers/leakage\n",
        "    \"id.orig_h\",\"id.resp_h\",\"id.orig_p\",\"id.resp_p\",\n",
        "    \"SrcAddr\",\"DstAddr\",\"SrcMac\",\"DstMac\",\"Packet_num\",\n",
        "    # protocol/state-ish categoricals you don't want as raw IDs\n",
        "    \"proto\",\"Proto\",\"service\",\"conn_state\",\"history\",\"tunnel_parents\",\n",
        "    \"State\",\"Flgs\",\"Dir\",\n",
        "    # labels/meta\n",
        "    \"traffic\",\"is_attack\",\"Label\",\"label\",\"class\",\"scenario\",\"Label_binary\",\n",
        "    \"Attack Category\",\"Attack_Category\",\"StartTime\"\n",
        "])\n",
        "\n",
        "DROP_BIOMETRICS = True\n",
        "BIOMETRIC_COLS = {\n",
        "    \"temp\",\"spo2\",\"pulse_rate\",\"sys\",\"dia\",\"heart_rate\",\"resp_rate\",\"st\"\n",
        "}\n",
        "SMALL_ONE_HOTS = [\"proto\",\"Proto\",\"service\",\"conn_state\",\"state\",\"State\",\n",
        "                  \"dir\",\"Dir\",\"Flgs\",\"local_orig\",\"local_resp\"]\n",
        "\n",
        "def curate_features(df):\n",
        "    df = df.copy()\n",
        "    present_small = [c for c in SMALL_ONE_HOTS if c in df.columns]\n",
        "    if present_small:\n",
        "        df = pd.get_dummies(df, columns=present_small, dummy_na=False)\n",
        "\n",
        "    block = set()\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        if c in NEVER_FEATURES or c in EXPLICIT_BLOCK:\n",
        "            block.add(c); continue\n",
        "        if TIME_LIKE_PAT.search(c):\n",
        "            block.add(c); continue\n",
        "        if ID_LIKE_PAT.search(c):\n",
        "            block.add(c); continue\n",
        "        # NEW: drop WUSTL-like biometrics if requested\n",
        "        if DROP_BIOMETRICS and lc in BIOMETRIC_COLS:\n",
        "            block.add(c); continue\n",
        "\n",
        "    candidates = [c for c in df.columns if c not in block]\n",
        "    cand_num = [c for c in candidates if np.issubdtype(df[c].dtype, np.number)]\n",
        "    assert len(cand_num) > 0, \"No numeric feature columns remained after curation.\"\n",
        "    return df, cand_num\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# columns whose names match this may legitimately contain negatives\n",
        "_NEG_ALLOW_RX = re.compile(r\"(diff|delta|change|zscore|(^|[_\\.])z($|[_\\.])|resid|residual|signed|skew)\", re.I)\n",
        "\n",
        "def negative_value_audit(df, feature_cols):\n",
        "    \"\"\"\n",
        "    Report columns that should typically be non-negative but have negatives.\n",
        "    Skips columns whose names suggest signed quantities (diff/z/resid/etc.).\n",
        "    Returns a dict: {col: {\"negatives\": n, \"total\": N, \"frac\": n/N, \"min\": min_negative}}\n",
        "    \"\"\"\n",
        "    rep = {}\n",
        "    for c in feature_cols:\n",
        "        if _NEG_ALLOW_RX.search(c):\n",
        "            continue\n",
        "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        s = s[~s.isna()]\n",
        "        if s.empty:\n",
        "            continue\n",
        "        neg_mask = s < 0\n",
        "        if neg_mask.any():\n",
        "            vneg = s[neg_mask]\n",
        "            rep[c] = {\n",
        "                \"negatives\": int(neg_mask.sum()),\n",
        "                \"total\": int(s.size),\n",
        "                \"frac\": float(neg_mask.mean()),\n",
        "                \"min\": float(vneg.min()),\n",
        "            }\n",
        "    return rep\n",
        "\n"
      ],
      "metadata": {
        "id": "IrOblEgMGhZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Val/Test splitting (temporal + balanced fallback)**\n",
        "We first split by row order (flows in file sequence) to preserve temporal structure and avoid look-ahead. We also leak-check using a content_hash so exact duplicate flows don’t appear across splits.\n",
        "However, two datasets out of 4 had a long contiguous regions of a single class, which can make val/test nearly all positives or all negatives. When that happens, we switch to a chunked, class-balanced split: we cut the two datasets into contiguous chunks, compute each chunk’s positive rate, then distribute high/low chunks round-robin into train/val/test (with retries and smaller chunks) until val/test class balance is reasonable and each split is large enough to form at least one window."
      ],
      "metadata": {
        "id": "kU0lIxCVV_EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell5\n",
        "def temporal_split_indices(df, test_frac=0.15, val_frac=0.05):\n",
        "    n = len(df)\n",
        "    i_test = int(n*(1-test_frac))\n",
        "    i_val  = int(n*(1-test_frac-val_frac))\n",
        "    return slice(0,i_val), slice(i_val,i_test), slice(i_test,n)\n",
        "\n",
        "def make_splits(df):\n",
        "    # Always split by row index (flow order)\n",
        "    tr, va, te = temporal_split_indices(df)\n",
        "    idx = {\"train\": tr, \"val\": va, \"test\": te}\n",
        "\n",
        "    # leak check\n",
        "    leak = {}\n",
        "    if \"content_hash\" in df.columns:\n",
        "        for a, b in [(\"train\",\"val\"),(\"train\",\"test\"),(\"val\",\"test\")]:\n",
        "            s = set(df.iloc[idx[a]][\"content_hash\"]).intersection(\n",
        "                  set(df.iloc[idx[b]][\"content_hash\"]))\n",
        "            leak[f\"{a}_x_{b}\"] = len(s)\n",
        "    return idx, leak\n",
        "\n",
        "#   chunk-stratified split (balanced + retries)\n",
        "\n",
        "def _ranges_to_index(ranges):\n",
        "    out = []\n",
        "    for a, b in ranges:\n",
        "        out.extend(range(a, b))\n",
        "    return np.array(out, dtype=int)\n",
        "\n",
        "def _split_sizes(n, val_frac, test_frac):\n",
        "    n_val  = int(round(n * val_frac))\n",
        "    n_test = int(round(n * test_frac))\n",
        "    n_train = n - n_val - n_test\n",
        "    return n_train, n_val, n_test\n",
        "\n",
        "def _pos_rate_from_index(y_all, ix):\n",
        "    arr = y_all[ix] if isinstance(ix, slice) else y_all[np.asarray(ix)]\n",
        "    return float(arr.mean()) if arr.size else 0.0, int(arr.size)\n",
        "\n",
        "def make_splits_chunked(df, val_frac=0.05, test_frac=0.15, chunk_rows=20_000, seed=42,\n",
        "                        pos_extreme=0.98, min_chunk_rows=5_000, max_retries=3):\n",
        "    \"\"\"\n",
        "    Balanced chunked split:\n",
        "      - make contiguous chunks [a,b)\n",
        "      - compute pos-rate per chunk\n",
        "      - distribute chunks in round-robin across (train, val, test) from high & low ends\n",
        "      - retry with smaller chunk_rows if any split is near-all-positive/negative\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    y_all = df[\"Label_binary\"].astype(\"int8\").to_numpy()\n",
        "    n = len(df)\n",
        "    n_train, n_val, n_test = _split_sizes(n, val_frac, test_frac)\n",
        "    targets = {\"train\": n_train, \"val\": n_val, \"test\": n_test}\n",
        "    order_cycle = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "    for attempt in range(max_retries + 1):\n",
        "        # build chunks\n",
        "        starts = list(range(0, n, chunk_rows))\n",
        "        chunks = [(s, min(s + chunk_rows, n)) for s in starts]\n",
        "\n",
        "        # compute pos-rate per chunk\n",
        "        per_chunk = []\n",
        "        for (a, b) in chunks:\n",
        "            seg = y_all[a:b]\n",
        "            pr = float(seg.mean()) if seg.size else 0.0\n",
        "            per_chunk.append((pr, a, b))\n",
        "\n",
        "        # sort by pos-rate\n",
        "        per_chunk.sort(key=lambda t: t[0])   # low .. high\n",
        "        lo, hi = 0, len(per_chunk) - 1\n",
        "\n",
        "        # assign in round-robin high/low fashion\n",
        "        sel = {\"train\": [], \"val\": [], \"test\": []}\n",
        "        used = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "        pick_hi = True\n",
        "        cyc_idx = 0\n",
        "\n",
        "        def _room(split):\n",
        "            return max(0, targets[split] - used[split])\n",
        "\n",
        "        while lo <= hi:\n",
        "            pr, a, b = per_chunk[hi] if pick_hi else per_chunk[lo]\n",
        "            pick_hi = not pick_hi\n",
        "            span = b - a\n",
        "\n",
        "            # try up to 3 different next-splits to place without overrunning target too much\n",
        "            tried = 0\n",
        "            placed = False\n",
        "            while tried < 3 and not placed:\n",
        "                split = order_cycle[cyc_idx % 3]\n",
        "                cyc_idx += 1\n",
        "                tried += 1\n",
        "                room = _room(split)\n",
        "                # allow small overshoot (<= half a chunk) to finish targets\n",
        "                if room >= span or (room == 0 and used[split] < targets[split] + chunk_rows // 2):\n",
        "                    sel[split].append((a, b))\n",
        "                    used[split] += span\n",
        "                    placed = True\n",
        "\n",
        "            if not placed:\n",
        "                # force into the split with max remaining room\n",
        "                split = max(order_cycle, key=lambda s: _room(s))\n",
        "                sel[split].append((a, b))\n",
        "                used[split] += span\n",
        "\n",
        "            if pick_hi:\n",
        "                hi -= 1\n",
        "            else:\n",
        "                lo += 1\n",
        "\n",
        "        # materialize indices\n",
        "        idx = {k: _ranges_to_index(v) for k, v in sel.items()}\n",
        "\n",
        "        # quick pre-window diagnostics\n",
        "        pr_tr, n_tr = _pos_rate_from_index(y_all, idx[\"train\"])\n",
        "        pr_va, n_va = _pos_rate_from_index(y_all, idx[\"val\"])\n",
        "        pr_te, n_te = _pos_rate_from_index(y_all, idx[\"test\"])\n",
        "        print(f\"[split] pre-window pos-rate  train={pr_tr:.3f} (n={n_tr})  \"\n",
        "              f\"val={pr_va:.3f} (n={n_va})  test={pr_te:.3f} (n={n_te})\")\n",
        "\n",
        "        # sanity: each split must have at least one window\n",
        "        min_win = W + max(HORIZONS) + 1\n",
        "        ok_sizes = (n_tr >= min_win) and (n_va >= min_win) and (n_te >= min_win)\n",
        "        lo, hi = (1 - pos_extreme), pos_extreme\n",
        "        ok_balance = (lo <= pr_va <= hi) and (lo <= pr_te <= hi)\n",
        "\n",
        "\n",
        "        if ok_sizes and ok_balance:\n",
        "            # leak check via content_hash (optional)\n",
        "            leak = {}\n",
        "            if \"content_hash\" in df.columns:\n",
        "                for a, b in [(\"train\",\"val\"), (\"train\",\"test\"), (\"val\",\"test\")]:\n",
        "                    s = set(df.iloc[idx[a]][\"content_hash\"]).intersection(\n",
        "                        set(df.iloc[idx[b]][\"content_hash\"])\n",
        "                    )\n",
        "                    leak[f\"{a}_x_{b}\"] = len(s)\n",
        "            return idx, leak\n",
        "\n",
        "        # retry with smaller chunk_rows\n",
        "        if attempt < max_retries and chunk_rows > min_chunk_rows:\n",
        "            chunk_rows = max(min_chunk_rows, chunk_rows // 2)\n",
        "            print(f\"[split] retrying with smaller CHUNK_ROWS={chunk_rows} \"\n",
        "                  f\"(attempt {attempt+1}/{max_retries})\")\n",
        "        else:\n",
        "            print(\"[split] Could not obtain balanced chunked splits within retries; \"\n",
        "                  \"falling back to contiguous.\")\n",
        "            return make_splits(df)  # fallback\n",
        "\n"
      ],
      "metadata": {
        "id": "3-zbc7jCGkHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputation, variance filtering, clipping, and scaling**\n",
        "We handle feature preprocessing in three stages:\n",
        "\n",
        "Imputation: Missing values are filled with training-set medians (applied consistently to val/test) to avoid information leakage.\n",
        "\n",
        "Variance filtering: Features with zero variance in the training split are dropped, since they carry no predictive signal.\n",
        "\n",
        "Clipping & scaling: Each feature is clipped to the [0.5%, 99.5%] quantile range (robust against outliers), then transformed with a RobustScaler to normalize distributions while reducing sensitivity to extreme values."
      ],
      "metadata": {
        "id": "ZMtd-Jd_ZTWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell6\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "def train_only_impute(df_tr, df_va, df_te, feature_cols):\n",
        "    med = df_tr[feature_cols].median()\n",
        "    df_tr[feature_cols] = df_tr[feature_cols].fillna(med)\n",
        "    df_va[feature_cols] = df_va[feature_cols].fillna(med)\n",
        "    df_te[feature_cols] = df_te[feature_cols].fillna(med)\n",
        "    return med, (df_tr, df_va, df_te)\n",
        "\n",
        "def drop_zero_var(df_tr, df_va, df_te, feature_cols):\n",
        "    keep = [c for c in feature_cols if df_tr[c].nunique(dropna=True) > 1]\n",
        "    return keep, (df_tr[keep], df_va[keep], df_te[keep])\n",
        "\n",
        "def clip_and_scale(df_tr, df_va, df_te, feature_cols, qlow=CLIP_Q_LOW, qhigh=CLIP_Q_HIGH):\n",
        "    ql = df_tr[feature_cols].quantile(qlow)\n",
        "    qh = df_tr[feature_cols].quantile(qhigh)\n",
        "    # nudge degenerate bounds\n",
        "    for c in feature_cols:\n",
        "        if qh[c] <= ql[c]:\n",
        "            qh[c] = ql[c] + 1e-9\n",
        "    def _clip(df):\n",
        "        vals = df[feature_cols].to_numpy(dtype=np.float64, copy=True)\n",
        "        low  = ql[feature_cols].astype(float).to_numpy()\n",
        "        high = qh[feature_cols].astype(float).to_numpy()\n",
        "        # Broadcast clip per-column\n",
        "        np.clip(vals, low, high, out=vals)\n",
        "        out = df.copy()\n",
        "        out[feature_cols] = vals.astype(np.float32, copy=False)\n",
        "        return out\n",
        "    df_tr_c = _clip(df_tr); df_va_c = _clip(df_va); df_te_c = _clip(df_te)\n",
        "    scaler = RobustScaler()\n",
        "    Xtr = scaler.fit_transform(df_tr_c[feature_cols].values.astype(\"float32\"))\n",
        "    Xva = scaler.transform(df_va_c[feature_cols].values.astype(\"float32\"))\n",
        "    Xte = scaler.transform(df_te_c[feature_cols].values.astype(\"float32\"))\n",
        "    assert np.isfinite(Xtr).all() and np.isfinite(Xva).all() and np.isfinite(Xte).all()\n",
        "    return scaler, (Xtr, Xva, Xte), {\"clip_low\": ql.to_dict(), \"clip_high\": qh.to_dict()}\n"
      ],
      "metadata": {
        "id": "JcBPunq5GmcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sliding-window labeling with earliness (K):**\n",
        "We convert row-wise features into fixed-length sequences for sequence models. For each time index t, we take the past W rows as one window X[t-W:t]. For each prediction horizon h ∈ {1,5,10}, the label Y[h] is 1 if any positive event occurs in the next h steps (look-ahead), else 0. We also record K[h]: the step (1..h) of the first positive within that horizon, or −1 if none—this supports earliness/lead-time analysis. The loop advances by stride (default 1). Outputs: a tensor X of shape (num_windows, W, F), dicts Y[h] (int8) and K[h] (int16) aligned per window."
      ],
      "metadata": {
        "id": "h479SFj3bONB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell7\n",
        "\n",
        "def build_windows_by_rows_with_k(X_mat, y_vec, W=50, horizons=(1,5,10), stride=1):\n",
        "    N, F = X_mat.shape\n",
        "    Hmax = max(horizons)\n",
        "    out_X, outY = [], {h: [] for h in horizons}\n",
        "    outK = {h: [] for h in horizons}  # 1..h to first attack; -1 if none\n",
        "    t = W\n",
        "    while t + Hmax <= N:\n",
        "        out_X.append(X_mat[t-W:t])\n",
        "        future = y_vec[t:t+Hmax]\n",
        "        for h in horizons:\n",
        "            seg = future[:h]\n",
        "            y = int((seg > 0).any())\n",
        "            k = (np.argmax(seg > 0) + 1) if y == 1 else -1\n",
        "            outY[h].append(y)\n",
        "            outK[h].append(k)\n",
        "        t += stride\n",
        "    X = np.stack(out_X).astype(\"float32\") if out_X else np.empty((0, W, F), np.float32)\n",
        "    Y = {h: np.array(outY[h], dtype=\"int8\") for h in horizons}\n",
        "    K = {h: np.array(outK[h], dtype=\"int16\") for h in horizons}\n",
        "    return X, Y, K\n"
      ],
      "metadata": {
        "id": "H48W3VYLGolx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**integrates all the earlier preprocessing steps into a single pipeline.**\n",
        "the cell below is a helper functions (label inference, feature curation, duplicate audit, splitting, imputation, scaling, and windowing) in sequence, adds guardrails for data balance and minimum window size, and finally saves processed datasets, audits, and metadata."
      ],
      "metadata": {
        "id": "dF98StJ_dh3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell8\n",
        "import pickle, json\n",
        "\n",
        "def process_client(csv_path, out_dir, dataset_name):\n",
        "    print(f\"\\n=== {dataset_name} ===\\nReading {csv_path}\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    df = read_csv_robust(csv_path)\n",
        "    df = add_base_meta(df, csv_path)\n",
        "\n",
        "    # labels\n",
        "    y_bin, y_raw_col = infer_binary_label(df)\n",
        "    df[\"Label_binary\"] = y_bin\n",
        "    if y_raw_col not in (\"Label_binary\",):\n",
        "        df[\"Label_raw\"] = df[y_raw_col]\n",
        "    # ensure binary labels are strictly 0/1\n",
        "    if not set(np.unique(df[\"Label_binary\"].dropna().values)).issubset({0,1}):\n",
        "     raise ValueError(\"Label_binary must be strictly 0/1.\")\n",
        "    # DO NOT reorder by time: keep original row order for flow windows\n",
        "    time_col = None\n",
        "\n",
        "    # coerce common numerics (best-effort; ok if absent)\n",
        "    # coerce common numerics (best-effort; ok if absent)\n",
        "    numeric_coerce_cols(df, [\n",
        "      # IoMT traffic / CICFlowMeter / Zeek\n",
        "      \"duration\",\"flow_duration\",\"Dur\",\"orig_bytes\",\"resp_bytes\",\"TotBytes\",\"tot_bytes\",\n",
        "      \"TotPkts\",\"fwd_pkts_tot\",\"bwd_pkts_tot\",\n",
        "      \"fwd_pkts_per_sec\",\"bwd_pkts_per_sec\",\"flow_pkts_per_sec\",\n",
        "      \"fwd_header_size_tot\",\"bwd_header_size_tot\",\"down_up_ratio\",\n",
        "      \"payload_bytes_per_second\",\n",
        "      # Argus/CLE/WUSTL variants\n",
        "      \"SrcBytes\",\"DstBytes\",\"SrcPkts\",\"Load\",\"SrcLoad\",\"Rate\",\n",
        "      \"SIntPkt\",\"DIntPkt\",\"SrcGap\",\"DstGap\",\"Loss\",\"pLoss\",\"pSrcLoss\",\"pDstLoss\",\n",
        "      \"sMaxPktSz\",\"dMaxPktSz\",\"sMinPktSz\",\"dMinPktSz\",\"Trans\"\n",
        "   ])\n",
        "\n",
        "    # pre-clean audit\n",
        "    pre = missing_inf_report(df)\n",
        "\n",
        "    # ports → buckets; curate feature set; fix inf\n",
        "    df = apply_port_flags(df)\n",
        "    df, feature_cols = curate_features(df)\n",
        "    for c in feature_cols:\n",
        "        df[c] = df[c].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # audit-only (no row removal): flows can repeat; we keep all rows\n",
        "    aud_path = os.path.join(out_dir, \"duplicates_report.csv\")\n",
        "    df_dedup, removed, dup_audit = audit_duplicates_no_drop(\n",
        "        df, feature_cols, label_col=\"Label_binary\", audit_path=aud_path\n",
        "    )\n",
        "\n",
        "    n_conflict = int((dup_audit['nlabels'] > 1).sum()) if dup_audit is not None else 0\n",
        "    print(f\"[audit] duplicate groups (no removal): {len(dup_audit)} | \"\n",
        "          f\"groups with mixed labels: {n_conflict} | rows removed: {removed}\")\n",
        "    print(f\"[audit] wrote per-hash stats to {aud_path}\")\n",
        "\n",
        "\n",
        "\n",
        "    # ---- choose split mode (contiguous / chunked / auto) ----\n",
        "    if SPLIT_MODE == \"CONTIG\":\n",
        "        idx, leak = make_splits(df_dedup)\n",
        "    elif SPLIT_MODE == \"CHUNKED\":\n",
        "        idx, leak = make_splits_chunked(\n",
        "            df_dedup, val_frac=VAL_FRAC, test_frac=TEST_FRAC,\n",
        "            chunk_rows=CHUNK_ROWS, seed=42,\n",
        "            pos_extreme=POS_EXTREME, min_chunk_rows=MIN_CHUNK_ROWS, max_retries=MAX_CHUNK_RETRIES\n",
        "        )\n",
        "    else:\n",
        "        # AUTO: try CONTIG, switch to CHUNKED if extreme\n",
        "        idx, leak = make_splits(df_dedup)\n",
        "        y_all = df_dedup[\"Label_binary\"].astype(\"int8\").to_numpy()\n",
        "        def _pr(ix):\n",
        "            arr = y_all[ix] if isinstance(ix, slice) else y_all[np.asarray(ix)]\n",
        "            return float(arr.mean()) if arr.size else 0.0\n",
        "        if (_pr(idx[\"val\"]) >= POS_EXTREME or _pr(idx[\"val\"]) <= 1-POS_EXTREME or\n",
        "            _pr(idx[\"test\"]) >= POS_EXTREME or _pr(idx[\"test\"]) <= 1-POS_EXTREME):\n",
        "            print(\"[split] AUTO switching to CHUNKED due to extreme val/test balance.\")\n",
        "            idx, leak = make_splits_chunked(\n",
        "                df_dedup, val_frac=VAL_FRAC, test_frac=TEST_FRAC,\n",
        "                chunk_rows=CHUNK_ROWS, seed=42,\n",
        "                pos_extreme=POS_EXTREME, min_chunk_rows=MIN_CHUNK_ROWS, max_retries=MAX_CHUNK_RETRIES\n",
        "            )\n",
        "\n",
        "\n",
        "    # ---- ensure each split has enough rows to form at least one window ----\n",
        "    def _len_ix(ix):\n",
        "        return (ix.stop - ix.start) if isinstance(ix, slice) else int(np.asarray(ix).size)\n",
        "\n",
        "    min_win = W + max(HORIZONS) + 1  # minimum rows needed to form a window\n",
        "    n_tr0, n_va0, n_te0 = _len_ix(idx[\"train\"]), _len_ix(idx[\"val\"]), _len_ix(idx[\"test\"])\n",
        "\n",
        "    if (n_tr0 < min_win) or (n_va0 < min_win) or (n_te0 < min_win):\n",
        "        print(f\"[split] Selected split produced too-small split(s) \"\n",
        "              f\"(train={n_tr0}, val={n_va0}, test={n_te0}, need≥{min_win}). \"\n",
        "              f\"Retrying CHUNKED with smaller CHUNK_ROWS...\")\n",
        "\n",
        "        retry_chunk = max(min_win, CHUNK_ROWS // 2)\n",
        "        idx, leak = make_splits_chunked(\n",
        "            df_dedup, val_frac=VAL_FRAC, test_frac=TEST_FRAC,\n",
        "            chunk_rows=retry_chunk, seed=42\n",
        "        )\n",
        "\n",
        "        n_tr0, n_va0, n_te0 = _len_ix(idx[\"train\"]), _len_ix(idx[\"val\"]), _len_ix(idx[\"test\"])\n",
        "        if (n_tr0 < min_win) or (n_va0 < min_win) or (n_te0 < min_win):\n",
        "            print(f\"[split] Retry still too small (train={n_tr0}, val={n_va0}, test={n_te0}). \"\n",
        "                  f\"Falling back to CONTIG for safety.\")\n",
        "            idx, leak = make_splits(df_dedup)\n",
        "        y_all = df_dedup[\"Label_binary\"].astype(\"int8\").to_numpy()\n",
        "        def _len_ix(ix): return (ix.stop - ix.start) if isinstance(ix, slice) else int(np.asarray(ix).size)\n",
        "        def _pos(ix):\n",
        "            arr = y_all[ix] if isinstance(ix, slice) else y_all[np.asarray(ix)]\n",
        "            return float(arr.mean()) if arr.size else 0.0, arr.size\n",
        "\n",
        "        pr_tr,n_tr = _pos(idx[\"train\"]); pr_va,n_va = _pos(idx[\"val\"]); pr_te,n_te = _pos(idx[\"test\"])\n",
        "        print(f\"[split] pre-window pos-rate  train={pr_tr:.3f} (n={n_tr})  val={pr_va:.3f} (n={n_va})  test={pr_te:.3f} (n={n_te})\")\n",
        "\n",
        "\n",
        "    # materialize splits\n",
        "    tr = df_dedup.iloc[idx[\"train\"]].copy()\n",
        "    va = df_dedup.iloc[idx[\"val\"]].copy()\n",
        "    te = df_dedup.iloc[idx[\"test\"]].copy()\n",
        "\n",
        "\n",
        "    # --- Guardrail 1: ensure each split can form ≥1 window ---\n",
        "    min_win = W + max(HORIZONS) + 1            # e.g., 50 + 10 + 1 = 61\n",
        "    for name, part in [(\"train\", tr), (\"val\", va), (\"test\", te)]:\n",
        "        if len(part) < min_win:\n",
        "            raise ValueError(\n",
        "                f\"{dataset_name}:{name} split too small for windowing — \"\n",
        "                f\"need ≥ {min_win} rows, have {len(part)}.\"\n",
        "            )\n",
        "    # train-only impute\n",
        "    med, (tr, va, te) = train_only_impute(tr, va, te, feature_cols)\n",
        "\n",
        "    # drop zero-variance (decided on train); keep full frames\n",
        "    keep_cols, _ = drop_zero_var(tr, va, te, feature_cols)\n",
        "    feature_cols = keep_cols\n",
        "\n",
        "    # clip+scale\n",
        "    scaler, (Xtr_raw, Xva_raw, Xte_raw), clip_bounds = clip_and_scale(tr, va, te, feature_cols)\n",
        "\n",
        "    # negative value audit\n",
        "    neg_audit = negative_value_audit(tr, feature_cols)\n",
        "\n",
        "    # save audits\n",
        "    post = missing_inf_report(df_dedup)\n",
        "    with open(os.path.join(out_dir, \"missing_inf_report_preclean.json\"), \"w\") as f: json.dump(pre, f, indent=2)\n",
        "    with open(os.path.join(out_dir, \"missing_inf_report_postclean.json\"), \"w\") as f: json.dump(post, f, indent=2)\n",
        "    with open(os.path.join(out_dir, \"negative_values_report.json\"), \"w\") as f: json.dump(neg_audit, f, indent=2)\n",
        "    with open(os.path.join(out_dir, \"cross_split_leak_report.json\"), \"w\") as f: json.dump(leak, f, indent=2)\n",
        "    with open(os.path.join(out_dir, \"feature_columns.txt\"), \"w\") as f: f.write(\"\\n\".join(feature_cols))\n",
        "\n",
        "    # build ROW windows (now also returning K = distance-to-event)\n",
        "    def split_to_windows_with_k(df_split, X_split):\n",
        "        y_vec = df_split[\"Label_binary\"].values.astype(\"int8\")\n",
        "        return build_windows_by_rows_with_k(X_split, y_vec, W=W, horizons=HORIZONS, stride=STRIDE)\n",
        "\n",
        "    Xtr_win, Ytr, Ktr = split_to_windows_with_k(tr, Xtr_raw)\n",
        "    Xva_win, Yva, Kva = split_to_windows_with_k(va, Xva_raw)\n",
        "    Xte_win, Yte, Kte = split_to_windows_with_k(te, Xte_raw)\n",
        "\n",
        "    # --- Persist exact split indices (relative to df_dedup) ---\n",
        "    def _slice_to_list(slc):\n",
        "        if isinstance(slc, slice):\n",
        "            return list(range(slc.start, slc.stop))\n",
        "        # handle numpy/pandas indexers too\n",
        "        return [int(i) for i in (slc if hasattr(slc, \"__iter__\") else [int(slc)])]\n",
        "\n",
        "\n",
        "    split_indices = {\n",
        "      \"train\": _slice_to_list(idx[\"train\"]),\n",
        "      \"val\":   _slice_to_list(idx[\"val\"]),\n",
        "      \"test\":  _slice_to_list(idx[\"test\"]),\n",
        "      }\n",
        "    with open(os.path.join(out_dir, \"split_indices.json\"), \"w\") as f:\n",
        "       json.dump(split_indices, f, indent=2)\n",
        "    # --- Guardrail 2a: X/Y alignment sanity ---\n",
        "    n_tr, n_va, n_te = Xtr_win.shape[0], Xva_win.shape[0], Xte_win.shape[0]\n",
        "    for h in HORIZONS:\n",
        "        assert len(Ytr[h]) == n_tr, f\"h={h} train y len {len(Ytr[h])} != X len {n_tr}\"\n",
        "        assert len(Yva[h]) == n_va, f\"h={h} val   y len {len(Yva[h])} != X len {n_va}\"\n",
        "        assert len(Yte[h]) == n_te, f\"h={h} test  y len {len(Yte[h])} != X len {n_te}\"\n",
        "\n",
        "    # --- Guardrail 2b: quick balance audit (just prints) ---\n",
        "    def frac(y): return float((y > 0).mean()) if len(y) else 0.0\n",
        "    print(\"Positive fractions by horizon (train | val | test):\")\n",
        "    for h in HORIZONS:\n",
        "        print(f\"  h={h}: {frac(Ytr[h]):.3f} | {frac(Yva[h]):.3f} | {frac(Yte[h]):.3f}\")\n",
        "\n",
        "    def _pos_frac(y):\n",
        "     y = y.astype(\"int8\")\n",
        "     return float((y == 1).mean()) if len(y) else 0.0\n",
        "\n",
        "    class_weights = {}\n",
        "    for split, Y in [(\"train\", Ytr), (\"val\", Yva), (\"test\", Yte)]:\n",
        "        cw = {}\n",
        "        for h in HORIZONS:\n",
        "            p = _pos_frac(Y[h])\n",
        "            # inverse-prevalence for positives, capped for stability\n",
        "            w_pos = float(min(50.0, (1.0 - p) / max(1e-6, p))) if p > 0 else 1.0\n",
        "            cw[int(h)] = {\"w_neg\": 1.0, \"w_pos\": w_pos}\n",
        "        class_weights[split] = cw\n",
        "\n",
        "    with open(os.path.join(out_dir, \"class_weights.json\"), \"w\") as f:\n",
        "        json.dump(class_weights, f, indent=2)\n",
        "    def save_split(name, X, Y, K):\n",
        "        if SAVE_AS_MEMMAP:\n",
        "            np.save(os.path.join(out_dir, f\"X_{name}.npy\"), X.astype(\"float32\"))\n",
        "            for h in HORIZONS:\n",
        "                np.save(os.path.join(out_dir, f\"y_{h}_{name}.npy\"), Y[h].astype(\"int8\"))\n",
        "                np.save(os.path.join(out_dir, f\"k_{h}_{name}.npy\"), K[h].astype(\"int16\"))\n",
        "        else:\n",
        "            np.savez_compressed(os.path.join(out_dir, f\"X_{name}.npz\"), X=X.astype(\"float32\"))\n",
        "            for h in HORIZONS:\n",
        "                np.save(os.path.join(out_dir, f\"y_{h}_{name}.npy\"), Y[h].astype(\"int8\"))\n",
        "                np.save(os.path.join(out_dir, f\"k_{h}_{name}.npy\"), K[h].astype(\"int16\"))\n",
        "\n",
        "    save_split(\"train\", Xtr_win, Ytr, Ktr)\n",
        "    save_split(\"val\",   Xva_win, Yva, Kva)\n",
        "    save_split(\"test\",  Xte_win, Yte, Kte)\n",
        "\n",
        "\n",
        "    # reports\n",
        "    horizon_balance = {\n",
        "        \"train\": {int(h): int(Ytr[h].sum()) for h in HORIZONS},\n",
        "        \"val\":   {int(h): int(Yva[h].sum()) for h in HORIZONS},\n",
        "        \"test\":  {int(h): int(Yte[h].sum()) for h in HORIZONS}\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"horizon_balance.json\"), \"w\") as f: json.dump(horizon_balance, f, indent=2)\n",
        "\n",
        "    artifacts = {\n",
        "        \"scaler\": scaler,\n",
        "        \"clip_bounds\": clip_bounds,\n",
        "        \"feature_cols\": feature_cols,\n",
        "        \"medians\": med.to_dict(),\n",
        "        \"W\": W, \"HORIZONS\": list(HORIZONS), \"STRIDE\": STRIDE\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"artifacts.pkl\"), \"wb\") as f: pickle.dump(artifacts, f)\n",
        "\n",
        "    meta = {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"n_features\": len(feature_cols),\n",
        "        \"W\": W, \"horizons\": list(HORIZONS), \"stride\": STRIDE,\n",
        "        \"shapes\": {\n",
        "            \"train\": [int(x) for x in Xtr_win.shape],\n",
        "            \"val\":   [int(x) for x in Xva_win.shape],\n",
        "            \"test\":  [int(x) for x in Xte_win.shape]\n",
        "        },\n",
        "        \"leak_report\": leak,\n",
        "        \"split_mode\": SPLIT_MODE,\n",
        "        \"chunk_rows\": CHUNK_ROWS,\n",
        "        \"val_frac\": VAL_FRAC,\n",
        "        \"test_frac\": TEST_FRAC\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(out_dir, \"meta.json\"), \"w\") as f: json.dump(meta, f, indent=2)\n",
        "\n",
        "    print(f\"{dataset_name}: X_train {Xtr_win.shape}, X_val {Xva_win.shape}, X_test {Xte_win.shape}\")\n",
        "    print(\"Horizon positives (train):\", {h:int(Ytr[h].sum()) for h in HORIZONS})\n",
        "    if RUN_EDA:\n",
        "      run_minimal_eda(df_dedup, idx, out_dir, feature_cols, corr_sample=5000)\n",
        "\n"
      ],
      "metadata": {
        "id": "eA1ZZk-fGtzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA (sanity visuals & quick stats)**\n",
        "using the cell bellow we generates a lightweight EDA pack per client split to catch issues early. It writes a text summary (rows, kept features, split sizes), plots class balance (linear & log scale), shows top-missing features, a few train feature histograms (auto log if heavy-tailed), a small correlation heatmap (sampled), and an “attack-rate over file order” line to reveal drift. If Label_raw exists, it also saves a top-30 class snapshot and a binary-vs-raw crosstab. Outputs go to <out_dir>/EDA/."
      ],
      "metadata": {
        "id": "ZQtAswWCeKmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 8.1 — Minimal EDA figs to <out_dir>/EDA/\n",
        "import matplotlib.pyplot as plt\n",
        "import os, math, numpy as np, pandas as pd\n",
        "\n",
        "# ---- toggles for optional EDA bits ----\n",
        "SHOW_FEATURE_HISTS = True     # set False to skip histograms\n",
        "MAX_FEAT_PLOTS     = 6        # how many features to show in hist figure\n",
        "SHOW_LABEL_RAW     = True     # set False to skip label_raw plots/crosstab\n",
        "\n",
        "def run_minimal_eda(df_full, idx, out_dir, feature_cols, corr_sample=5000, seed=7):\n",
        "    eda_dir = os.path.join(out_dir, \"EDA\")\n",
        "    os.makedirs(eda_dir, exist_ok=True)\n",
        "\n",
        "    # 0) small text summary\n",
        "    with open(os.path.join(eda_dir, \"summary.txt\"), \"w\") as f:\n",
        "        f.write(f\"rows={len(df_full)}\\nfeatures_kept={len(feature_cols)}\\n\")\n",
        "        for split in (\"train\",\"val\",\"test\"):\n",
        "            sl = idx[split]\n",
        "            n = len(df_full.iloc[sl]) if isinstance(sl, slice) else len(sl)\n",
        "            y = df_full.loc[sl, \"Label_binary\"].astype(\"int64\")\n",
        "            f.write(f\"{split}: n={n}, pos={int((y==1).sum())}, neg={int((y==0).sum())}\\n\")\n",
        "\n",
        "    # 1) Class balance (linear & log) per split\n",
        "    for name, slc in idx.items():\n",
        "        y = df_full.loc[slc, \"Label_binary\"].values\n",
        "        pos = int((y == 1).sum()); neg = int((y == 0).sum())\n",
        "        for scale, tag in [(\"linear\",\"\"), (\"log\",\"_log\")]:\n",
        "            plt.figure()\n",
        "            plt.bar([\"neg\",\"pos\"], [max(0,neg), max(0,pos)])\n",
        "            if scale == \"log\": plt.yscale(\"log\")\n",
        "            plt.title(f\"class_balance_{name} (neg={neg}, pos={pos})\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(eda_dir, f\"class_balance_{name}{tag}.png\"), dpi=150)\n",
        "            plt.close()\n",
        "\n",
        "    # 2) Missingness (top-40 by NaN count)\n",
        "    na_counts = df_full[feature_cols].isna().sum()\n",
        "    na_counts = na_counts[na_counts > 0].sort_values(ascending=False).head(40)\n",
        "    if len(na_counts) > 0:\n",
        "        plt.figure(figsize=(10, max(3, 0.28*len(na_counts))))\n",
        "        plt.barh(na_counts.index.tolist(), na_counts.values)\n",
        "        plt.title(\"missingness_top40\")\n",
        "        plt.xlabel(\"NaN count\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(eda_dir, \"missingness_top40.png\"), dpi=150)\n",
        "        plt.close()\n",
        "    else:\n",
        "        with open(os.path.join(eda_dir, \"missingness_top40.txt\"), \"w\") as f:\n",
        "            f.write(\"No missing values in feature columns.\\n\")\n",
        "\n",
        "    # 3) Distributions for a few train features (optional; auto log-hist if heavy skew)\n",
        "    if SHOW_FEATURE_HISTS:\n",
        "        tr = df_full.loc[idx[\"train\"], feature_cols]\n",
        "        feats = feature_cols[:MAX_FEAT_PLOTS]\n",
        "        cols = 2; rows = max(1, math.ceil(len(feats)/cols))\n",
        "        plt.figure(figsize=(11, 3.2*rows))\n",
        "        rng = np.random.default_rng(seed)\n",
        "        for i, c in enumerate(feats, 1):\n",
        "            s = pd.to_numeric(tr[c], errors=\"coerce\").dropna()\n",
        "            if len(s) > 200000:   # cap for big datasets\n",
        "                s = pd.Series(rng.choice(s.values, size=200000, replace=False))\n",
        "            plt.subplot(rows, cols, i)\n",
        "            if len(s) == 0:\n",
        "                plt.title(f\"{c} (no data)\"); plt.hist([]); continue\n",
        "            if s.min() >= 0 and (s.replace(0, np.nan).min(skipna=True) is not None):\n",
        "                mn = s.replace(0, np.nan).min(skipna=True)\n",
        "                if mn is not None and s.max()/max(1e-9, mn) > 1e3:\n",
        "                    s = s.replace(0, np.nan).dropna()\n",
        "                    plt.hist(np.log10(s.values), bins=60)\n",
        "                    plt.xlabel(\"log10(value)\")\n",
        "                else:\n",
        "                    plt.hist(s.values, bins=60)\n",
        "            else:\n",
        "                plt.hist(s.values, bins=60)\n",
        "            plt.title(c, fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(eda_dir, \"feature_hist_train.png\"), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # 4) Correlation heatmap on a sample of train (numeric only)\n",
        "    tr = df_full.loc[idx[\"train\"], feature_cols]\n",
        "    if len(tr) > 1:\n",
        "        n_samp = min(corr_sample, len(tr))\n",
        "        sample = tr.sample(n_samp, random_state=seed)\n",
        "        C = sample.corr(numeric_only=True)\n",
        "        plt.figure(figsize=(min(12, 0.25*C.shape[1]+4), min(10, 0.25*C.shape[0]+4)))\n",
        "        plt.imshow(C.values, aspect='auto', interpolation='nearest')\n",
        "        plt.colorbar()\n",
        "        plt.title(\"corr_train_sample\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(eda_dir, \"corr_train_sample.png\"), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # 5) Attack-rate drift over file order (per client file)\n",
        "    chunk = max(1000, min(10000, len(df_full)//100))  # ~100 points\n",
        "    if chunk > 0:\n",
        "        y = df_full[\"Label_binary\"].astype(\"int8\").values\n",
        "        pts, rates = [], []\n",
        "        for i in range(0, len(y), chunk):\n",
        "            seg = y[i:i+chunk]\n",
        "            pts.append(i + len(seg)//2)\n",
        "            rates.append(float((seg==1).mean()))\n",
        "        plt.figure(figsize=(10,3))\n",
        "        plt.plot(pts, rates, marker='.')\n",
        "        plt.title(f\"attack_rate_by_position (chunks≈{chunk} rows)\")\n",
        "        plt.xlabel(\"row index (file order)\")\n",
        "        plt.ylabel(\"positive rate\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(eda_dir, \"attack_rate_by_position.png\"), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # 6) Multi-class snapshot (only if present AND enabled)\n",
        "    if SHOW_LABEL_RAW and (\"Label_raw\" in df_full.columns):\n",
        "        lab = (df_full[\"Label_raw\"].astype(str).str.strip().str.lower()\n",
        "               .replace({\"\": \"<empty>\", \"nan\": \"<nan>\"}))\n",
        "        vc = lab.value_counts()\n",
        "        if len(vc) > 0:\n",
        "            vc_top = vc.head(30)\n",
        "            plt.figure(figsize=(10, max(3, 0.28*len(vc_top))))\n",
        "            plt.barh(vc_top.index[::-1], vc_top.values[::-1])\n",
        "            plt.title(\"label_raw_distribution_top30\")\n",
        "            plt.xlabel(\"count\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(eda_dir, \"label_raw_distribution_top30.png\"), dpi=150)\n",
        "            plt.close()\n",
        "            pd.crosstab(df_full[\"Label_binary\"].astype(int), lab, dropna=False) \\\n",
        "              .to_csv(os.path.join(eda_dir, \"Label_binary_by_Label_raw.csv\"))\n"
      ],
      "metadata": {
        "id": "4Xhv0AYVra1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below is a  helper verifies dataset integrity after preprocessing. It loads metadata and all saved arrays (X, y, and optionally k) for each split (train/val/test) and horizon (1, 5, 10). It asserts alignment (same number of samples across features, labels, and distance-to-event k), ensuring no mismatches before training."
      ],
      "metadata": {
        "id": "ce6no1l1jwFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sanity(out_dir):\n",
        "    with open(os.path.join(out_dir, \"meta.json\")) as f:\n",
        "        meta = json.load(f)\n",
        "    print(\"Shapes:\", meta[\"shapes\"])\n",
        "    for split in [\"train\",\"val\",\"test\"]:\n",
        "        X = np.load(os.path.join(out_dir, f\"X_{split}.npy\"), mmap_mode=\"r\")\n",
        "        n = X.shape[0]\n",
        "        for h in (1,5,10):\n",
        "            y = np.load(os.path.join(out_dir, f\"y_{h}_{split}.npy\"))\n",
        "            assert len(y) == n, (split, h, \"y len\", len(y), \"X len\", n)\n",
        "            k_path = os.path.join(out_dir, f\"k_{h}_{split}.npy\")\n",
        "            if os.path.exists(k_path):\n",
        "                k = np.load(k_path)\n",
        "                assert len(k) == n, (split, h, \"k len\", len(k), \"X len\", n)\n",
        "    print(\" aligned for all splits/horizons (and K if present)\")\n"
      ],
      "metadata": {
        "id": "p6j3UeUEfgRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing results for each dataset"
      ],
      "metadata": {
        "id": "ncDAHrcmj2Gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "client 1 : ice ransomware dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "ucDBGg4hLuop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_client(ICE_CSV, OUT_ICE, DATASET_NAME)\n",
        "sanity(OUT_ICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp5J1kZD80JE",
        "outputId": "d52e89e0-5777-4db6-bdcc-b6ee316a3691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ICE ===\n",
            "Reading /content/drive/MyDrive/fed_MID/iomt_clients/client_ICE.csv\n",
            "[audit] duplicate groups (no removal): 507300 | groups with mixed labels: 0 | rows removed: 0\n",
            "[audit] wrote per-hash stats to /content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/ICE/duplicates_report.csv\n",
            "Positive fractions by horizon (train | val | test):\n",
            "  h=1: 0.217 | 0.338 | 0.302\n",
            "  h=5: 0.268 | 0.502 | 0.538\n",
            "  h=10: 0.278 | 0.609 | 0.589\n",
            "ICE: X_train (405780, 50, 15), X_val (25307, 50, 15), X_test (76036, 50, 15)\n",
            "Horizon positives (train): {1: 88204, 5: 108858, 10: 112650}\n",
            "Shapes: {'train': [405780, 50, 15], 'val': [25307, 50, 15], 'test': [76036, 50, 15]}\n",
            " aligned for all splits/horizons (and K if present)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALIDATION WAS ACROSS ALL THE DATA"
      ],
      "metadata": {
        "id": "GfJd4Trrrocg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: validator\n",
        "import os, json, numpy as np, glob\n",
        "\n",
        "def validate_npys(out_dir, horizons=(1,5,10), W=50):\n",
        "    problems = []\n",
        "    # 1) presence\n",
        "    need = [\"meta.json\",\"artifacts.pkl\",\"feature_columns.txt\",\"split_indices.json\",\"class_weights.json\"]\n",
        "    for f in need:\n",
        "        if not os.path.exists(os.path.join(out_dir, f)):\n",
        "            problems.append(f\"[missing] {f}\")\n",
        "\n",
        "    # 2) shapes & alignment\n",
        "    with open(os.path.join(out_dir,\"meta.json\")) as f:\n",
        "        meta = json.load(f)\n",
        "    shapes = meta[\"shapes\"]\n",
        "    for split in [\"train\",\"val\",\"test\"]:\n",
        "        X = np.load(os.path.join(out_dir, f\"X_{split}.npy\"), mmap_mode=\"r\")\n",
        "        if X.ndim != 3 or X.shape[1] != W:\n",
        "            problems.append(f\"[shape] X_{split} bad shape {X.shape}, expect (?,{W},F)\")\n",
        "        for h in horizons:\n",
        "            y = np.load(os.path.join(out_dir, f\"y_{h}_{split}.npy\"))\n",
        "            if len(y) != X.shape[0]:\n",
        "                problems.append(f\"[align] y_{h}_{split} len={len(y)} != X_{split} n={X.shape[0]}\")\n",
        "\n",
        "    # 3) label sanity (prevalence and monotonicity)\n",
        "    def _prev(p): return float((p>0).mean()) if len(p) else 0.0\n",
        "    for split in [\"train\",\"val\",\"test\"]:\n",
        "        ys = [np.load(os.path.join(out_dir, f\"y_{h}_{split}.npy\")) for h in horizons]\n",
        "        prevs = [_prev(y) for y in ys]\n",
        "        if any(p==0 or p==1 for p in prevs):\n",
        "            problems.append(f\"[labels] {split} extreme class ratio {prevs}\")\n",
        "        # monotonic: y_h must be <= y_h' if h<h'\n",
        "        for i in range(len(horizons)-1):\n",
        "            if np.any((ys[i]==1) & (ys[i+1]==0)):\n",
        "                problems.append(f\"[labels] non-monotonic: h={horizons[i]} > h={horizons[i+1]} in {split}\")\n",
        "\n",
        "    # 4) feature names (just warn if time/IDs slipped through)\n",
        "    bad_tokens = (\"time\",\"timestamp\",\"starttime\",\"uid\",\"guid\",\"mac\",\"addr\",\"sha\",\"hash\",\"patient\",\"nhs\")\n",
        "    with open(os.path.join(out_dir,\"feature_columns.txt\")) as f:\n",
        "        cols = [c.strip().lower() for c in f]\n",
        "    bad = [c for c in cols if any(tok in c for tok in bad_tokens)]\n",
        "    if bad:\n",
        "        problems.append(f\"[leak?] suspicious features present: {bad[:10]}{'...' if len(bad)>10 else ''}\")\n",
        "\n",
        "    # summary\n",
        "    if problems:\n",
        "        print(f\"[VALIDATOR] Issues in {out_dir}:\")\n",
        "        for p in problems: print(\" -\", p)\n",
        "    else:\n",
        "        print(f\"[VALIDATOR] OK: {out_dir}\")\n",
        "\n",
        "# Run for each client dir you produced\n",
        "validate_npys(OUT_ICE, horizons=(1,5,10), W=50)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DU8iCZOAVR6",
        "outputId": "e0d1f899-daf7-45f6-b695-670a04fea681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VALIDATOR] OK: /content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/ICE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "client 2 :  iomt-a (IoMT-TrafficData)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gkd9WFlcLoLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_client(IOMT_A_CSV, OUT_IOMT_A, DATASET_NAME)\n",
        "sanity(OUT_IOMT_A)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcddc075-eb45-4603-acdd-f011865c8322",
        "id": "2YCblxWPK8-5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== IOMT_A ===\n",
            "Reading /content/drive/MyDrive/fed_MID/iomt_clients/client_iomt_A.csv\n",
            "[audit] duplicate groups (no removal): 852920 | groups with mixed labels: 5 | rows removed: 0\n",
            "[audit] wrote per-hash stats to /content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/IOMT_A/duplicates_report.csv\n",
            "[split] AUTO switching to CHUNKED due to extreme val/test balance.\n",
            "[split] pre-window pos-rate  train=0.794 (n=1343076)  val=0.500 (n=80000)  test=0.231 (n=260000)\n",
            "Positive fractions by horizon (train | val | test):\n",
            "  h=1: 0.794 | 0.500 | 0.231\n",
            "  h=5: 0.794 | 0.500 | 0.231\n",
            "  h=10: 0.794 | 0.500 | 0.231\n",
            "IOMT_A: X_train (1343017, 50, 55), X_val (79941, 50, 55), X_test (259941, 50, 55)\n",
            "Horizon positives (train): {1: 1065978, 5: 1066034, 10: 1066104}\n",
            "Shapes: {'train': [1343017, 50, 55], 'val': [79941, 50, 55], 'test': [259941, 50, 55]}\n",
            " aligned for all splits/horizons (and K if present)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "client 3 :  iomt-b (IoMT-TrafficData)"
      ],
      "metadata": {
        "id": "88vLyJziLkoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_client(IOMT_B_CSV, OUT_IOMT_B, DATASET_NAME)\n",
        "sanity(OUT_IOMT_B)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf403d5-b5f2-42aa-ea63-1fe75aa619ec",
        "id": "pnA7tvE4LMSQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== IOMT_B ===\n",
            "Reading /content/drive/MyDrive/fed_MID/iomt_clients/client_iomt_B.csv\n",
            "[audit] duplicate groups (no removal): 436558 | groups with mixed labels: 2 | rows removed: 0\n",
            "[audit] wrote per-hash stats to /content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/IOMT_B/duplicates_report.csv\n",
            "[split] AUTO switching to CHUNKED due to extreme val/test balance.\n",
            "[split] pre-window pos-rate  train=0.879 (n=1240112)  val=0.600 (n=100000)  test=0.909 (n=220000)\n",
            "Positive fractions by horizon (train | val | test):\n",
            "  h=1: 0.879 | 0.600 | 0.909\n",
            "  h=5: 0.879 | 0.600 | 0.909\n",
            "  h=10: 0.879 | 0.600 | 0.909\n",
            "IOMT_B: X_train (1240053, 50, 58), X_val (99941, 50, 58), X_test (219941, 50, 58)\n",
            "Horizon positives (train): {1: 1090177, 5: 1090189, 10: 1090204}\n",
            "Shapes: {'train': [1240053, 50, 58], 'val': [99941, 50, 58], 'test': [219941, 50, 58]}\n",
            " aligned for all splits/horizons (and K if present)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "client 4:  WUSTL(wustl-ehms-2020)"
      ],
      "metadata": {
        "id": "sjy2X14ALiK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_client(WUSTL_CSV, OUT_WUSTL, DATASET_NAME)\n",
        "sanity(OUT_WUSTL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e510ff30-7a11-47fc-a5db-51018d007e2e",
        "id": "sZxh6Qe9LfSF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== WUSTL ===\n",
            "Reading /content/drive/MyDrive/fed_MID/iomt_clients/client_WUSTL.csv\n",
            "[audit] duplicate groups (no removal): 16318 | groups with mixed labels: 0 | rows removed: 0\n",
            "[audit] wrote per-hash stats to /content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY/WUSTL/duplicates_report.csv\n",
            "Positive fractions by horizon (train | val | test):\n",
            "  h=1: 0.126 | 0.125 | 0.127\n",
            "  h=5: 0.147 | 0.141 | 0.147\n",
            "  h=10: 0.173 | 0.161 | 0.172\n",
            "WUSTL: X_train (12995, 50, 18), X_val (757, 50, 18), X_test (2389, 50, 18)\n",
            "Horizon positives (train): {1: 1641, 5: 1913, 10: 2253}\n",
            "Shapes: {'train': [12995, 50, 18], 'val': [757, 50, 18], 'test': [2389, 50, 18]}\n",
            " aligned for all splits/horizons (and K if present)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEDERATED LEARNING AND LOCAL MODELS"
      ],
      "metadata": {
        "id": "9VOOUvoEn4Aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Installs & imports the NEEDED LIBARARIIES (Flower, PyTorch, sklearn, matplotlib) and mounts Google Drive.\n",
        "\n",
        "Sets project paths to the preprocessed client datasets (ICE, IOMT_A/B, WUSTL) plus config/results folders.\n",
        "\n",
        "Fixes randomness  \n",
        "\n",
        "Defines globals: window size W=50, prediction HORIZONS=[1,5,10], device, and training hyperparams (EPOCHS_LOCAL, BATCH, LR, GRAD_CLIP).\n",
        "\n",
        "Early stopping policy: monitor validation PRAUC (or AUROC), with patience=2 and min_delta=1e-3, optionally verbose.\n",
        "\n",
        "Federated simulation config: 4 clients, 7 rounds × 5 local epochs per round, plus optional robust aggregation knobs (FED_TRIMMED_BETA) and FedProx strength (FED_FEDPROX_MU).\n",
        "\n",
        "Client name/index maps for convenient referencing during simulation."
      ],
      "metadata": {
        "id": "Uhe9gsWknEmj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kevJCIUUe1f_"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"flwr[simulation]\" torch torchvision pyyaml scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell1  Core\n",
        "import os, json, yaml, math, gc, pathlib, random\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Metrics/plots\n",
        "from sklearn.metrics import (\n",
        "    f1_score, roc_auc_score, average_precision_score,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flower\n",
        "import flwr\n",
        "from flwr.app import Context\n",
        "from flwr.client import NumPyClient, Client\n",
        "from flwr.clientapp import ClientApp\n",
        "\n",
        "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
        "from flwr.server.strategy import FedAvg, FedProx\n",
        "from flwr.simulation import run_simulation\n",
        "from flwr.common import ndarrays_to_parameters, parameters_to_ndarrays\n",
        "\n",
        "# Colab Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ------------------ Paths ------------------\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY\"\n",
        "CLIENTS = {\n",
        "    \"ICE\":     os.path.join(DRIVE_ROOT, \"ICE\"),\n",
        "    \"IOMT_A\":  os.path.join(DRIVE_ROOT, \"IOMT_A\"),\n",
        "    \"IOMT_B\":  os.path.join(DRIVE_ROOT, \"IOMT_B\"),\n",
        "    \"WUSTL\":   os.path.join(DRIVE_ROOT, \"WUSTL\"),\n",
        "}\n",
        "\n",
        "CFG_DIR = \"/content/drive/MyDrive/fed_MID/configs\"\n",
        "CLIENT_CFG_DIR = os.path.join(CFG_DIR, \"clients\")\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/fed_MID/results\"\n",
        "for d in [CFG_DIR, CLIENT_CFG_DIR, RESULTS_DIR]:\n",
        "    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ------------------ Globals ------------------\n",
        "HORIZONS = [1, 5, 10]\n",
        "W = 50\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training budgets\n",
        "EPOCHS_LOCAL = 20                 # each deep model (local-only & centralized)\n",
        "BATCH = 128\n",
        "LR = 1e-3\n",
        "GRAD_CLIP = 1.0\n",
        "# ---- Early Stopping (validation-based) ----\n",
        "ES_USE = True          # master toggle\n",
        "ES_PATIENCE = 2        # epochs with no improvement before stopping\n",
        "ES_MIN_DELTA = 1e-3    # min improvement to reset patience\n",
        "ES_MONITOR = \"prauc\"   # \"prauc\"  or \"auroc\"\n",
        "VERBOSE_ES = True      # print per-epoch train/val and overfit warnings\n",
        "\n",
        "# Federated (simulation)\n",
        "FED_NUM_CLIENTS = 4\n",
        "FED_NUM_ROUNDS = 7\n",
        "FED_EPOCHS_PER_ROUND = 5\n",
        "FED_TRIMMED_BETA = 0.10\n",
        "FED_FEDPROX_MU = 0.01  # proximal strength (client-side)\n",
        "\n",
        "# Make client→index maps\n",
        "CLIENT_NAMES = list(CLIENTS.keys())\n",
        "PID_TO_NAME = {i: n for i, n in enumerate(CLIENT_NAMES)}\n",
        "NAME_TO_PID = {n: i for i, n in PID_TO_NAME.items()}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "364sEjEF3wZa",
        "outputId": "6ad90227-b1b1-4b59-a235-b2dfb22807ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature alignment (centralized preprocessing)\n",
        "Builds the union of feature names across all client sites and rewrites each site’s X_* arrays to that common order (missing features zero-filled). Copies labels/metadata when present, updates meta.json with shapes and n_features, and writes via chunked memmaps for scalability—producing aligned datasets under ALIGNED/."
      ],
      "metadata": {
        "id": "qnR_v4YoUkpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " FEATURE ALIGNMENT (ONLY centralized)\n",
        "import os, json, numpy as np, pathlib, shutil\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/fed_MID/DATA_CLIENT_NPY\"\n",
        "SRC_DIRS = {\n",
        "    \"ICE\":   f\"{DRIVE_ROOT}/ICE\",\n",
        "    \"IOMT_A\":f\"{DRIVE_ROOT}/IOMT_A\",\n",
        "    \"IOMT_B\":f\"{DRIVE_ROOT}/IOMT_B\",\n",
        "    \"WUSTL\": f\"{DRIVE_ROOT}/WUSTL\",\n",
        "}\n",
        "ALIGNED_ROOT = f\"{DRIVE_ROOT}/ALIGNED\"     # destination for aligned copies\n",
        "pathlib.Path(ALIGNED_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Build union of feature names across all clients\n",
        "feat_lists = {}\n",
        "for name, d in SRC_DIRS.items():\n",
        "    with open(os.path.join(d, \"feature_columns.txt\")) as f:\n",
        "        feat_lists[name] = [c.strip() for c in f if c.strip()]\n",
        "feat_union = sorted(set().union(*feat_lists.values()))\n",
        "print(\"Union feature count:\", len(feat_union))\n",
        "\n",
        "def repack_to(dest_dir, src_dir, union_cols, W=50, splits=(\"train\",\"val\",\"test\"), chunk=4000):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    # Copy metadata/audits if present (optional; skipped if missing)\n",
        "    meta_like = [\n",
        "        \"class_weights.json\",\"split_indices.json\",\"artifacts.pkl\",\n",
        "        \"missing_inf_report_preclean.json\",\"missing_inf_report_postclean.json\",\n",
        "        \"negative_values_report.json\",\"horizon_balance.json\",\n",
        "        \"meta.json\",\"cross_split_leak_report.json\",\"leak_report.json\"\n",
        "    ]\n",
        "    for fname in meta_like:\n",
        "        src = os.path.join(src_dir, fname)\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy2(src, dest_dir)\n",
        "\n",
        "    # Copy labels (and k_*) as-is\n",
        "    for split in splits:\n",
        "        for h in (1,5,10):\n",
        "            for base in [f\"y_{h}_{split}.npy\", f\"k_{h}_{split}.npy\"]:\n",
        "                s = os.path.join(src_dir, base)\n",
        "                if os.path.exists(s):\n",
        "                    shutil.copy2(s, dest_dir)\n",
        "\n",
        "    # Map X_* to union\n",
        "    with open(os.path.join(src_dir, \"feature_columns.txt\")) as f:\n",
        "        old_cols = [c.strip() for c in f if c.strip()]\n",
        "    old_index = {c:i for i,c in enumerate(old_cols)}\n",
        "    idx_map = [old_index.get(c, -1) for c in union_cols]\n",
        "\n",
        "    for split in splits:\n",
        "        src_X = os.path.join(src_dir, f\"X_{split}.npy\")\n",
        "        X = np.load(src_X, mmap_mode=\"r\")  # (N, W, F_old)\n",
        "        N, W0, F_old = X.shape\n",
        "        assert W0 == W, f\"W mismatch in {src_X}: {W0} vs {W}\"\n",
        "        F_new = len(union_cols)\n",
        "        tmp = os.path.join(dest_dir, f\"X_{split}.npy.tmp\")\n",
        "        X_new = np.lib.format.open_memmap(tmp, mode=\"w+\", dtype=np.float32, shape=(N, W, F_new))\n",
        "        for a in range(0, N, chunk):\n",
        "            b = min(N, a+chunk)\n",
        "            block = X[a:b]                  # (B,W,F_old)\n",
        "            out = np.zeros((b-a, W, F_new), np.float32)\n",
        "            for j_new, j_old in enumerate(idx_map):\n",
        "                if j_old >= 0:\n",
        "                    out[:, :, j_new] = block[:, :, j_old]\n",
        "            X_new[a:b] = out\n",
        "        del X_new\n",
        "        os.replace(tmp, os.path.join(dest_dir, f\"X_{split}.npy\"))\n",
        "        print(f\"[{os.path.basename(dest_dir)}] {split}: {F_old}→{F_new}, N={N}\")\n",
        "\n",
        "    # Update feature list and meta (shapes)\n",
        "    with open(os.path.join(dest_dir, \"feature_columns.txt\"), \"w\") as f:\n",
        "        f.write(\"\\n\".join(union_cols))\n",
        "    meta_path = os.path.join(dest_dir, \"meta.json\")\n",
        "    meta = {}\n",
        "    if os.path.exists(meta_path):\n",
        "        with open(meta_path) as f:\n",
        "            try: meta = json.load(f)\n",
        "            except: meta = {}\n",
        "    meta.setdefault(\"shapes\", {})\n",
        "    for split in splits:\n",
        "        Xs = np.load(os.path.join(dest_dir, f\"X_{split}.npy\"), mmap_mode=\"r\")\n",
        "        meta[\"shapes\"][split] = [int(Xs.shape[0]), int(Xs.shape[1]), int(Xs.shape[2])]\n",
        "    meta[\"n_features\"] = len(union_cols)\n",
        "    with open(meta_path, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "# Run repack for each client into ALIGNED/*\n",
        "for name, src in SRC_DIRS.items():\n",
        "    repack_to(os.path.join(ALIGNED_ROOT, name), src, feat_union, W=50)\n",
        "\n",
        "print(\"Aligned copies ready under:\", ALIGNED_ROOT)\n"
      ],
      "metadata": {
        "id": "431O9rSro-KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data I/O and DataLoaders\n",
        " Loads per-split arrays (X_*, y_h_*, optional k_h_*), reads per-horizon class weights, wraps data in a windowed Dataset, and returns train/val/test DataLoaders plus the feature dimension required to instantiate models."
      ],
      "metadata": {
        "id": "c1MrN-B3U3rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_split_arrays(data_dir: str, split: str, horizons: List[int]):\n",
        "    X = np.load(os.path.join(data_dir, f\"X_{split}.npy\"), mmap_mode=\"r\")\n",
        "    ys = {h: np.load(os.path.join(data_dir, f\"y_{h}_{split}.npy\")) for h in horizons}\n",
        "    return X, ys\n",
        "\n",
        "def load_split_arrays_with_k(data_dir: str, split: str, horizons: List[int]):\n",
        "    X = np.load(os.path.join(data_dir, f\"X_{split}.npy\"), mmap_mode=\"r\")\n",
        "    ys = {h: np.load(os.path.join(data_dir, f\"y_{h}_{split}.npy\")) for h in horizons}\n",
        "    Ks = {}\n",
        "    for h in horizons:\n",
        "        p = os.path.join(data_dir, f\"k_{h}_{split}.npy\")\n",
        "        Ks[h] = np.load(p) if os.path.exists(p) else None\n",
        "    return X, ys, Ks\n",
        "\n",
        "def load_class_weights(data_dir: str, horizons: List[int]):\n",
        "    with open(os.path.join(data_dir, \"class_weights.json\")) as f:\n",
        "        cw = json.load(f)\n",
        "    # prefer \"train\" weights; fallback to 1.0\n",
        "    out = {}\n",
        "    for h in horizons:\n",
        "        key = str(h)\n",
        "        w = cw.get(\"train\", {}).get(key, {}).get(\"w_pos\", 1.0)\n",
        "        out[h] = float(w)\n",
        "    return out\n",
        "\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, X, Y_dict, horizons: List[int]):\n",
        "        self.X = np.asarray(X, dtype=np.float32)\n",
        "        self.horizons = list(horizons)\n",
        "        self.Y = np.stack([np.asarray(Y_dict[h], dtype=np.int64) for h in horizons], axis=1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #\n",
        "        x = np.array(self.X[idx], copy=True)\n",
        "        y = np.array(self.Y[idx], copy=True)\n",
        "        return torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "\n",
        "def make_loaders(data_dir, horizons, batch_size, shuffle_train=True):\n",
        "    Xtr, Ytr = load_split_arrays(data_dir, \"train\", horizons)\n",
        "    Xva, Yva = load_split_arrays(data_dir, \"val\",   horizons)\n",
        "    Xte, Yte = load_split_arrays(data_dir, \"test\",  horizons)\n",
        "    ds_tr = WindowDataset(Xtr, Ytr, horizons)\n",
        "    ds_va = WindowDataset(Xva, Yva, horizons)\n",
        "    ds_te = WindowDataset(Xte, Yte, horizons)\n",
        "\n",
        "\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=shuffle_train, num_workers=0, pin_memory=True)\n",
        "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False,          num_workers=0, pin_memory=True)\n",
        "    dl_te = DataLoader(ds_te, batch_size=batch_size, shuffle=False,          num_workers=0, pin_memory=True)\n",
        "    return dl_tr, dl_va, dl_te, Xtr.shape[2]\n"
      ],
      "metadata": {
        "id": "y_pMcum33-j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics, calibration, and plots\n",
        "Chooses decision thresholds on the validation set via macro-F1 grid search, computes evaluation metrics (macro-F1, AUROC, PR-AUC), estimates 95% bootstrap CIs, computes Expected Calibration Error (ECE), draws reliability diagrams, and provides precision@target-recall utilities and a safe savefig helper."
      ],
      "metadata": {
        "id": "MFCh7HlCVGG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pick_threshold_from_val(y_true, y_score):\n",
        "    thr_grid = np.linspace(0.05, 0.95, 19)\n",
        "    best_thr, best_f1 = 0.5, -1.0\n",
        "    y_true = np.asarray(y_true, int); y_score = np.asarray(y_score, float)\n",
        "    for t in thr_grid:\n",
        "        y_pred = (y_score >= t).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_thr = f1, t\n",
        "    return float(best_thr)\n",
        "\n",
        "def evaluate_split(y_true, y_score, threshold):\n",
        "    y_true = np.asarray(y_true, int); y_score = np.asarray(y_score, float)\n",
        "    y_pred = (y_score >= threshold).astype(int)\n",
        "    out = {}\n",
        "    out[\"F1_macro\"] = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
        "    try: out[\"AUROC\"] = float(roc_auc_score(y_true, y_score))\n",
        "    except Exception: out[\"AUROC\"] = float(\"nan\")\n",
        "    try: out[\"PR_AUC\"] = float(average_precision_score(y_true, y_score))\n",
        "    except Exception: out[\"PR_AUC\"] = float(\"nan\")\n",
        "    out[\"threshold\"] = float(threshold)\n",
        "    return out\n",
        "\n",
        "def bootstrap_ci(metric_fn, y_true, y_score, iters=1000, alpha=0.05, seed=SEED):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    y_true = np.asarray(y_true); y_score = np.asarray(y_score)\n",
        "    n = len(y_true)\n",
        "    vals = []\n",
        "    for _ in range(iters):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        vals.append(metric_fn(y_true[idx], y_score[idx]))\n",
        "    lo = float(np.quantile(vals, alpha/2)); hi = float(np.quantile(vals, 1-alpha/2))\n",
        "    return lo, hi\n",
        "\n",
        "def ece_score(y_true, y_score, n_bins=15):\n",
        "    y_true = np.asarray(y_true, int); y_score = np.asarray(y_score, float)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "    ece = 0.0; N = len(y_true)\n",
        "    for i in range(n_bins):\n",
        "        m = (y_score >= bins[i]) & (y_score < bins[i+1])\n",
        "        if m.any():\n",
        "            conf = y_score[m].mean()\n",
        "            acc  = ( (y_score[m] >= 0.5).astype(int) == y_true[m] ).mean()\n",
        "            w = m.mean()\n",
        "            ece += w * abs(acc - conf)\n",
        "    return float(ece)\n",
        "\n",
        "def plot_reliability(ax, y_true, y_score, n_bins=15, title=\"Calibration\"):\n",
        "    y_true = np.asarray(y_true, int); y_score = np.asarray(y_score, float)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "    xs, ys = [], []\n",
        "    for i in range(n_bins):\n",
        "        m = (y_score >= bins[i]) & (y_score < bins[i+1])\n",
        "        if m.any():\n",
        "            xs.append(y_score[m].mean())\n",
        "            ys.append((y_score[m] >= 0.5).astype(int).mean())\n",
        "    ax.plot([0,1],[0,1], linestyle=\"--\")\n",
        "    ax.plot(xs, ys, marker=\"o\")\n",
        "    ax.set_xlabel(\"Confidence\"); ax.set_ylabel(\"Accuracy\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "def precision_at_recall(y_true, y_score, target_recall=0.90):\n",
        "    p, r, _ = precision_recall_curve(y_true, y_score)\n",
        "    # pick highest precision where recall≥target\n",
        "    m = r >= target_recall\n",
        "    return float(p[m].max()) if m.any() else float(\"nan\")\n",
        "\n",
        "def savefig(path):\n",
        "    pathlib.Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True)\n",
        "    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()\n"
      ],
      "metadata": {
        "id": "k8j9W9hG4AZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: HIPFedPredict with attention (FedBN & private heads)\n",
        "Defines an additive-attention model: 1×1 projection → Conv-BN-ReLU×2 → BiLSTM → attention → per-horizon linear heads. Exposes FL-aware sharing so BatchNorm, the projection layer, and heads remain local/private (FedBN + private heads), while the rest of the trunk is shared via ndarray export/import."
      ],
      "metadata": {
        "id": "TyiffzHsVQQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(in_dim, in_dim, bias=True)\n",
        "        self.v = nn.Linear(in_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, seq):  # (B, T, C)\n",
        "        u = torch.tanh(self.W(seq))\n",
        "        s = self.v(u).squeeze(-1)              # (B, T)\n",
        "        a = torch.softmax(s, dim=1)            # (B, T)\n",
        "        c = (seq * a.unsqueeze(-1)).sum(1)     # (B, C)\n",
        "        return c, a\n",
        "\n",
        "\n",
        "class HIPFedPredict(nn.Module):\n",
        "    \"\"\"Projection (1x1 Conv) → Conv-BN-ReLU x2 → BiLSTM → Additive Attention → private heads.\"\"\"\n",
        "    def __init__(self, in_features, horizons: List[int]):\n",
        "        super().__init__()\n",
        "        self.horizons = list(horizons)\n",
        "\n",
        "        # projection on feature axis (treat (W,F) as (F,W) for Conv1d)\n",
        "        self.proj  = nn.Conv1d(in_channels=in_features, out_channels=128, kernel_size=1)\n",
        "        self.conv1 = nn.Conv1d(128, 128, kernel_size=3, padding=1, dilation=1)\n",
        "        self.bn1   = nn.BatchNorm1d(128)\n",
        "        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=2, dilation=2)\n",
        "        self.bn2   = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.bilstm = nn.LSTM(input_size=128, hidden_size=128, batch_first=True, bidirectional=True)\n",
        "        self.attn   = AdditiveAttention(in_dim=256)\n",
        "        self.heads  = nn.ModuleDict({str(h): nn.Linear(256, 1) for h in self.horizons})\n",
        "\n",
        "    def forward(self, x):           # x: (B, W, F)\n",
        "        z = x.transpose(1, 2)       # (B, F, W)\n",
        "        z = F.relu(self.bn1(self.conv1(self.proj(z))))   # (B,128,W)\n",
        "        z = F.relu(self.bn2(self.conv2(z)))              # (B,128,W)\n",
        "        z = z.transpose(1, 2)       # (B, W, 128)\n",
        "        z, _ = self.bilstm(z)       # (B, W, 256)\n",
        "        c, _ = self.attn(z)         # (B, 256)\n",
        "        return {h: self.heads[str(h)](c).squeeze(-1) for h in self.horizons}\n",
        "\n",
        "    # ----- FedBN + private heads: share everything EXCEPT BatchNorm and heads -----\n",
        "    def shared_keys(self):\n",
        "        bn_keys = set()\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.BatchNorm1d):\n",
        "                for k in module.state_dict().keys():\n",
        "                    bn_keys.add(f\"{name}.{k}\")\n",
        "                for p_name, _ in module.named_parameters(recurse=False):\n",
        "                    bn_keys.add(f\"{name}.{p_name}\")\n",
        "\n",
        "        keys = []\n",
        "        for k in self.state_dict().keys():\n",
        "            if k.startswith(\"heads.\"):   # keep heads private\n",
        "                continue\n",
        "            if k.startswith(\"proj.\"):    # keep projection local\n",
        "                continue\n",
        "            if k in bn_keys:             # FedBN: keep BN local\n",
        "                continue\n",
        "            keys.append(k)\n",
        "        return keys\n",
        "\n",
        "    def get_shared_ndarrays(self):\n",
        "        sd = self.state_dict()\n",
        "        keys = self.shared_keys()\n",
        "        arrs = [sd[k].detach().cpu().numpy() for k in keys]\n",
        "        return keys, arrs\n",
        "\n",
        "    def load_shared_ndarrays(self, keys, arrs, strict=False):\n",
        "        sd = self.state_dict()\n",
        "        for k, a in zip(keys, arrs):\n",
        "            if k in sd:\n",
        "                sd[k] = torch.from_numpy(a)\n",
        "        self.load_state_dict(sd, strict=strict)\n"
      ],
      "metadata": {
        "id": "AAVKBpgqe676"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & inference primitives (FedProx-ready)\n",
        "Implements one training epoch with per-horizon weighted BCE, optional FedProx proximal regularization against a global snapshot, and gradient clipping. Adds batched inference (predict_logits), validation summaries (avg AUROC/PR-AUC), and comprehensive per-horizon reporting (F1, AUROC/PR-AUC with 95% CIs, P@R=0.90, ECE, and “earliness” when k_h_* is available)"
      ],
      "metadata": {
        "id": "RtYUfNGiVdJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_pos_weight_tensor(pos_weight_scalar):\n",
        "    return torch.tensor([pos_weight_scalar], dtype=torch.float32, device=device)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, pos_weights: Dict[int, float],\n",
        "                grad_clip=1.0, fedprox_mu=0.0, global_state: Dict[str, torch.Tensor] = None):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)          # (B, W, F)\n",
        "        yb = yb.to(device)          # (B, H)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)          # dict h -> (B,)\n",
        "        loss = 0.0\n",
        "        for i, h in enumerate(model.horizons):\n",
        "            logit_h = logits[h]\n",
        "            target_h = yb[:, i].float()\n",
        "            pos_w = make_pos_weight_tensor(pos_weights[h])\n",
        "            loss += nn.BCEWithLogitsLoss(pos_weight=pos_w)(logit_h, target_h)\n",
        "        # FedProx proximal term (client-side)\n",
        "        if fedprox_mu > 0.0 and global_state is not None:\n",
        "            prox = 0.0\n",
        "            for name, p in model.named_parameters():\n",
        "                if p.requires_grad and (name in global_state):\n",
        "                    prox += ((p - global_state[name].to(device)) ** 2).sum()\n",
        "            loss = loss + (fedprox_mu / 2.0) * prox\n",
        "        loss.backward()\n",
        "        if grad_clip and grad_clip > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        total += float(loss.item())\n",
        "    return total / max(1, len(loader))\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_logits(model, loader):\n",
        "    model.eval()\n",
        "    scores = {h: [] for h in model.horizons}\n",
        "    truths = {h: [] for h in model.horizons}\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        for i, h in enumerate(model.horizons):\n",
        "            scores[h].append(torch.sigmoid(out[h]).cpu().numpy())\n",
        "            truths[h].append(yb[:, i].cpu().numpy())\n",
        "    scores = {h: np.concatenate(scores[h], 0) for h in model.horizons}\n",
        "    truths = {h: np.concatenate(truths[h], 0) for h in model.horizons}\n",
        "    return truths, scores\n",
        "\n",
        "def val_summary(model, dl_va):\n",
        "    \"\"\"Return (avg_pr_auc, avg_auroc) across horizons on validation loader.\"\"\"\n",
        "    yv, sv = predict_logits(model, dl_va)\n",
        "    pr, roc = [], []\n",
        "    for h in model.horizons:\n",
        "        try:\n",
        "            pr.append(average_precision_score(yv[h], sv[h]))\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            roc.append(roc_auc_score(yv[h], sv[h]))\n",
        "        except Exception:\n",
        "            pass\n",
        "    pr_avg  = float(np.mean(pr)) if pr else 0.0\n",
        "    roc_avg = float(np.mean(roc)) if roc else 0.0\n",
        "    return pr_avg, roc_avg\n",
        "\n",
        "def pack_metrics_per_horizon(horizons, y_true_dict, y_score_dict, val_thr_dict, k_test=None):\n",
        "    report = {}\n",
        "    for h in horizons:\n",
        "        y_true_h  = np.asarray(y_true_dict[h], dtype=int)\n",
        "        y_score_h = np.asarray(y_score_dict[h], dtype=float)\n",
        "\n",
        "        rep = evaluate_split(y_true_h, y_score_h, val_thr_dict[h])\n",
        "\n",
        "        # Precision@Recall=0.90\n",
        "        rep[\"Prec@R0.90\"] = precision_at_recall(y_true_h, y_score_h, 0.90)\n",
        "\n",
        "        # 95% CIs (bootstrap) for AUROC/PR-AUC\n",
        "        try:\n",
        "            lo, hi = bootstrap_ci(lambda yt, ys: roc_auc_score(yt, ys), y_true_h, y_score_h)\n",
        "            rep[\"AUROC_CI95\"] = [lo, hi]\n",
        "        except Exception:\n",
        "            rep[\"AUROC_CI95\"] = [\"nan\", \"nan\"]\n",
        "        try:\n",
        "            lo, hi = bootstrap_ci(lambda yt, ys: average_precision_score(yt, ys), y_true_h, y_score_h)\n",
        "            rep[\"PR_AUC_CI95\"] = [lo, hi]\n",
        "        except Exception:\n",
        "            rep[\"PR_AUC_CI95\"] = [\"nan\", \"nan\"]\n",
        "\n",
        "        # Expected Calibration Error\n",
        "        try:\n",
        "            rep[\"ECE\"] = ece_score(y_true_h, y_score_h)\n",
        "        except Exception:\n",
        "            rep[\"ECE\"] = \"nan\"\n",
        "\n",
        "        # Earliness (if k_{h}_* exists)\n",
        "        if k_test is not None and k_test.get(h) is not None:\n",
        "            y_pred_h = (y_score_h >= val_thr_dict[h]).astype(int)\n",
        "            k_arr = np.asarray(k_test[h], dtype=int)\n",
        "            m = (y_true_h == 1) & (y_pred_h == 1) & (k_arr > 0)\n",
        "            if m.any():\n",
        "                lead = (h - k_arr[m]) / float(h)\n",
        "                rep[\"Earliness\"] = float(np.clip(lead, 0.0, 1.0).mean())\n",
        "            else:\n",
        "                rep[\"Earliness\"] = \"N/A\"\n",
        "        else:\n",
        "            rep[\"Earliness\"] = \"N/A\"\n",
        "\n",
        "        report[str(h)] = rep\n",
        "\n",
        "    # Averages across horizons\n",
        "    for key in [\"F1_macro\", \"AUROC\", \"PR_AUC\", \"Prec@R0.90\"]:\n",
        "        vals = [report[str(h)][key] for h in horizons if isinstance(report[str(h)][key], (int, float))]\n",
        "        report[f\"_avg_{key}\"] = float(np.mean(vals)) if vals else \"N/A\"\n",
        "\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "qtZ6X-zZfki8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Local training, curves, and centralized oracle\n",
        "Trains a single-site model with early stopping on avg validation AUROC, saves learning-curve and calibration plots, and returns thresholds and a metrics report. Includes helpers to run all local baselines (results/local/) and to pool data across sites for a centralized “oracle” model with pooled class weights (results/centralized/)."
      ],
      "metadata": {
        "id": "r1Mt8iAcVnif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_json(d, path):\n",
        "    pathlib.Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(d, f, indent=2)\n",
        "\n",
        "# ---- helpers for validation/monitoring ----\n",
        "@torch.no_grad()\n",
        "def _val_metrics_avg(model, dl_va):\n",
        "    \"\"\"Compute threshold-free metrics on validation: avg AUROC/PR-AUC across horizons.\"\"\"\n",
        "    yv, sv = predict_logits(model, dl_va)\n",
        "    aurocs, praucs = [], []\n",
        "    for h in model.horizons:\n",
        "        yt = np.asarray(yv[h])\n",
        "        ys = np.asarray(sv[h])\n",
        "        try:\n",
        "            aurocs.append(roc_auc_score(yt, ys))\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            praucs.append(average_precision_score(yt, ys))\n",
        "        except Exception:\n",
        "            pass\n",
        "    au = float(np.mean(aurocs)) if aurocs else float(\"nan\")\n",
        "    pr = float(np.mean(praucs)) if praucs else float(\"nan\")\n",
        "    return au, pr, yv, sv  #\n",
        "def train_local_model(\n",
        "    data_dir,\n",
        "    horizons,\n",
        "    feature_dim,\n",
        "    epochs,\n",
        "    batch_size,\n",
        "    lr,\n",
        "    grad_clip,\n",
        "    class_pos_weights,\n",
        "    model_cls=HIPFedPredict,\n",
        "    fedprox_mu=0.0,\n",
        "    global_state=None,\n",
        "    # --- early stopping knobs ---\n",
        "    early_stop=True,\n",
        "    patience=4,\n",
        "    min_delta=1e-4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a single local model with early stopping on validation AUROC (higher is better).\n",
        "    Also logs per-epoch train loss and val AUROC/PR-AUC, saves curves to RESULTS_DIR.\n",
        "    \"\"\"\n",
        "    from copy import deepcopy\n",
        "\n",
        "    model = model_cls(in_features=feature_dim, horizons=horizons).to(device)\n",
        "    dl_tr, dl_va, dl_te, _ = make_loaders(data_dir, horizons, batch_size)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # histories for monitoring/plots\n",
        "    hist = {\"train_loss\": [], \"val_auroc_avg\": [], \"val_prauc_avg\": []}\n",
        "\n",
        "    # early-stopping state\n",
        "    best_metric = -float(\"inf\")  # monitor = val_auroc_avg (maximize)\n",
        "    best_state = None\n",
        "    best_epoch = -1\n",
        "    wait = 0\n",
        "\n",
        "    # ---- training loop ----\n",
        "    for ep in range(1, epochs + 1):\n",
        "        tr_loss = train_epoch(\n",
        "            model,\n",
        "            dl_tr,\n",
        "            opt,\n",
        "            class_pos_weights,\n",
        "            grad_clip=grad_clip,\n",
        "            fedprox_mu=fedprox_mu,\n",
        "            global_state=global_state,\n",
        "        )\n",
        "        hist[\"train_loss\"].append(float(tr_loss))\n",
        "\n",
        "        val_au, val_pr, _, _ = _val_metrics_avg(model, dl_va)\n",
        "        hist[\"val_auroc_avg\"].append(val_au)\n",
        "        hist[\"val_prauc_avg\"].append(val_pr)\n",
        "\n",
        "        # early stop on val_auroc_avg\n",
        "        improved = (not np.isnan(val_au)) and (val_au > best_metric + min_delta)\n",
        "        if improved:\n",
        "            best_metric = float(val_au)\n",
        "            best_state = deepcopy(model.state_dict())\n",
        "            best_epoch = ep\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "\n",
        "        # print small progress line (handy in Colab logs)\n",
        "        print(f\"[{model_cls.__name__}] ep {ep:02d}/{epochs} | train_loss={tr_loss:.4f} | \"\n",
        "              f\"val_AUROC={val_au:.4f} | val_PRAUC={val_pr:.4f} | best_ep={best_epoch} \", flush=True)\n",
        "\n",
        "        if early_stop and wait >= patience:\n",
        "            print(f\"[{model_cls.__name__}] Early stop at epoch {ep} (patience={patience}, best_ep={best_epoch})\")\n",
        "            break\n",
        "\n",
        "    # restore best weights (if we ever improved)\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state, strict=False)\n",
        "\n",
        "    # ---- choose thresholds on val (using the *current/best* model) ----\n",
        "    yv, sv = predict_logits(model, dl_va)\n",
        "    val_thr = {h: pick_threshold_from_val(yv[h], sv[h]) for h in horizons}\n",
        "\n",
        "    # ---- final test evaluation ----\n",
        "    yt, st = predict_logits(model, dl_te)\n",
        "    _, _, Kte = load_split_arrays_with_k(data_dir, \"test\", horizons)\n",
        "    report = pack_metrics_per_horizon(horizons, yt, st, val_thr, k_test=Kte)\n",
        "    report[\"_early_stop\"] = {\n",
        "        \"enabled\": bool(early_stop),\n",
        "        \"best_epoch\": int(best_epoch if best_epoch != -1 else len(hist[\"train_loss\"])),\n",
        "        \"monitor\": \"val_auroc_avg\",\n",
        "        \"best_val_auroc\": float(best_metric) if best_metric != -float(\"inf\") else \"nan\",\n",
        "        \"total_epochs_run\": int(len(hist[\"train_loss\"])),\n",
        "    }\n",
        "\n",
        "    # ---- save learning curves (per-client dir inferred from data_dir) ----\n",
        "    client_name = os.path.basename(data_dir.rstrip(\"/\"))\n",
        "    base_dir = os.path.join(RESULTS_DIR, \"local\", client_name)\n",
        "    pathlib.Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # JSON history\n",
        "    save_json(hist, os.path.join(base_dir, f\"{model_cls.__name__}_curves.json\"))\n",
        "\n",
        "    # Plots\n",
        "    try:\n",
        "        # Train loss\n",
        "        plt.figure()\n",
        "        plt.plot(range(1, len(hist[\"train_loss\"]) + 1), hist[\"train_loss\"], marker=\"o\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Train loss\"); plt.title(f\"{model_cls.__name__} — train loss\")\n",
        "        savefig(os.path.join(base_dir, f\"{model_cls.__name__}_train_loss.png\"))\n",
        "\n",
        "        # Val AUROC\n",
        "        plt.figure()\n",
        "        plt.plot(range(1, len(hist[\"val_auroc_avg\"]) + 1), hist[\"val_auroc_avg\"], marker=\"o\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Val AUROC (avg)\"); plt.title(f\"{model_cls.__name__} — val AUROC\")\n",
        "        savefig(os.path.join(base_dir, f\"{model_cls.__name__}_val_auroc.png\"))\n",
        "\n",
        "        # Val PR-AUC\n",
        "        plt.figure()\n",
        "        plt.plot(range(1, len(hist[\"val_prauc_avg\"]) + 1), hist[\"val_prauc_avg\"], marker=\"o\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Val PR-AUC (avg)\"); plt.title(f\"{model_cls.__name__} — val PR-AUC\")\n",
        "        savefig(os.path.join(base_dir, f\"{model_cls.__name__}_val_prauc.png\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # ---- calibration figs per horizon ----\n",
        "    for h in horizons:\n",
        "        plt.figure()\n",
        "        plot_reliability(plt.gca(), yt[h], st[h], title=f\"Calibration (h={h})\")\n",
        "        savefig(os.path.join(base_dir, f\"{model_cls.__name__}_calib_h{h}.png\"))\n",
        "\n",
        "\n",
        "    return model, report, val_thr\n",
        "\n",
        "def run_local_only_all():\n",
        "    summary = {}\n",
        "    for cname, cdir in CLIENTS.items():\n",
        "        print(f\"[LOCAL] {cname}\")\n",
        "        dl_tr, dl_va, dl_te, F = make_loaders(cdir, HORIZONS, BATCH)\n",
        "        posw = load_class_weights(cdir, HORIZONS)\n",
        "\n",
        "        res = {\"DL\": {}}\n",
        "\n",
        "        # Deep model\n",
        "        _, report, _ = train_local_model(\n",
        "            cdir,\n",
        "            HORIZONS,\n",
        "            F,\n",
        "            epochs=EPOCHS_LOCAL,\n",
        "            batch_size=BATCH,\n",
        "            lr=LR,\n",
        "            grad_clip=GRAD_CLIP,\n",
        "            class_pos_weights=posw,\n",
        "            model_cls=HIPFedPredict,\n",
        "            early_stop=True,\n",
        "            patience=4,\n",
        "            min_delta=1e-4,\n",
        "        )\n",
        "        res[\"DL\"][\"HIPFedPredict\"] = report\n",
        "\n",
        "        # Save per-client and accumulate\n",
        "        save_json(res, os.path.join(RESULTS_DIR, \"local\", cname, \"results.json\"))\n",
        "        summary[cname] = res\n",
        "        gc.collect()\n",
        "\n",
        "    save_json(summary, os.path.join(RESULTS_DIR, \"local\", \"summary.json\"))\n",
        "    print(\"[LOCAL] done.\")\n",
        "\n",
        "def run_centralized_oracle():\n",
        "    # pool train/val\n",
        "    Xtr_all, Ytr_all = [], {h: [] for h in HORIZONS}\n",
        "    Xva_all, Yva_all = [], {h: [] for h in HORIZONS}\n",
        "    pooled_F = None\n",
        "    for cdir in CLIENTS.values():\n",
        "        Xtr, Ytr = load_split_arrays(cdir, \"train\", HORIZONS)\n",
        "        Xva, Yva = load_split_arrays(cdir, \"val\", HORIZONS)\n",
        "        if pooled_F is None:\n",
        "            pooled_F = Xtr.shape[2]\n",
        "        Xtr_all.append(np.asarray(Xtr)); Xva_all.append(np.asarray(Xva))\n",
        "        for h in HORIZONS:\n",
        "            Ytr_all[h].append(Ytr[h]); Yva_all[h].append(Yva[h])\n",
        "    Xtr_all = np.concatenate(Xtr_all, 0); Xva_all = np.concatenate(Xva_all, 0)\n",
        "    Ytr_all = {h: np.concatenate(Ytr_all[h], 0) for h in HORIZONS}\n",
        "    Yva_all = {h: np.concatenate(Yva_all[h], 0) for h in HORIZONS}\n",
        "\n",
        "    POOL_DIR = \"/content/drive/MyDrive/fed_MID/_central_pooled\"\n",
        "    pathlib.Path(POOL_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    np.save(os.path.join(POOL_DIR, \"X_train.npy\"), Xtr_all.astype(np.float32))\n",
        "    np.save(os.path.join(POOL_DIR, \"X_val.npy\"),   Xva_all.astype(np.float32))\n",
        "    for h in HORIZONS:\n",
        "        np.save(os.path.join(POOL_DIR, f\"y_{h}_train.npy\"), Ytr_all[h].astype(np.int8))\n",
        "        np.save(os.path.join(POOL_DIR, f\"y_{h}_val.npy\"),   Yva_all[h].astype(np.int8))\n",
        "\n",
        "    # Pooled test = concat all tests\n",
        "    Xte_all, Yte_all = [], {h: [] for h in HORIZONS}\n",
        "    for cdir in CLIENTS.values():\n",
        "        Xte, Yte = load_split_arrays(cdir, \"test\", HORIZONS)\n",
        "        Xte_all.append(np.asarray(Xte))\n",
        "        for h in HORIZONS:\n",
        "            Yte_all[h].append(Yte[h])\n",
        "    Xte_all = np.concatenate(Xte_all, 0)\n",
        "    for h in HORIZONS:\n",
        "        Yte_all[h] = np.concatenate(Yte_all[h], 0)\n",
        "        np.save(os.path.join(POOL_DIR, f\"y_{h}_test.npy\"), Yte_all[h].astype(np.int8))\n",
        "    np.save(os.path.join(POOL_DIR, \"X_test.npy\"), Xte_all.astype(np.float32))\n",
        "\n",
        "    # pos weights from pooled train\n",
        "    pooled_posw = {}\n",
        "    for h in HORIZONS:\n",
        "        p = float(np.mean(Ytr_all[h]))\n",
        "        pooled_posw[h] = float(min(50.0, (1.0 - p) / max(1e-6, p))) if p > 0 else 1.0\n",
        "\n",
        "    res = {\"DL\": {}}\n",
        "    _, report, _ = train_local_model(\n",
        "        POOL_DIR,\n",
        "        HORIZONS,\n",
        "        pooled_F,\n",
        "        epochs=EPOCHS_LOCAL,\n",
        "        batch_size=BATCH,\n",
        "        lr=LR,\n",
        "        grad_clip=GRAD_CLIP,\n",
        "        class_pos_weights=pooled_posw,\n",
        "        model_cls=HIPFedPredict,\n",
        "        early_stop=True,\n",
        "        patience=4,\n",
        "        min_delta=1e-4,\n",
        "    )\n",
        "    res[\"DL\"][\"HIPFedPredict\"] = report\n",
        "\n",
        "    save_json(res, os.path.join(RESULTS_DIR, \"centralized\", \"results.json\"))\n",
        "    print(\"[CENTRALIZED] done.\")\n"
      ],
      "metadata": {
        "id": "fBzITtdgflE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flower client (NumPyClient)**\n",
        "Implements a per-site client: sets device, builds loaders and class weights, initializes HIPFedPredict+Adam, and defines FL hooks. Shares only permitted layers, trains locally (optionally with FedProx), tracks the best validation AUROC to return the best shared weights + metrics, and saves each client’s full final model (local BN + private heads)."
      ],
      "metadata": {
        "id": "Ar0Sky5uVv4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell9\n",
        "from flwr.common import parameters_to_ndarrays  # keep your existing imports\n",
        "\n",
        "class IoMTClient(NumPyClient):\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self.data_dir = CLIENTS[name]\n",
        "        self.horizons = HORIZONS\n",
        "        self.batch = BATCH\n",
        "        self.epochs = FED_EPOCHS_PER_ROUND\n",
        "        self.lr = LR\n",
        "        self.grad_clip = GRAD_CLIP\n",
        "        self.fedprox_mu = FED_FEDPROX_MU\n",
        "\n",
        "        # --- pick device inside the actor (not from a global) ---\n",
        "        _has_cuda = torch.cuda.is_available() and torch.cuda.device_count() > 0\n",
        "        self.device = torch.device(\"cuda\") if _has_cuda else torch.device(\"cpu\")\n",
        "\n",
        "        # data\n",
        "        self.dl_tr, self.dl_va, self.dl_te, self.F = make_loaders(\n",
        "            self.data_dir, self.horizons, self.batch\n",
        "        )\n",
        "        self.posw = load_class_weights(self.data_dir, self.horizons)\n",
        "\n",
        "        # model/opt\n",
        "        self.model = HIPFedPredict(self.F, self.horizons).to(self.device)\n",
        "        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        _, arrs = self.model.get_shared_ndarrays()\n",
        "        return arrs\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_incoming(parameters):\n",
        "        \"\"\"Accept either list[np.ndarray] or Flower Parameters; return list[np.ndarray] or None.\"\"\"\n",
        "        if parameters is None:\n",
        "            return None\n",
        "        if isinstance(parameters, list):\n",
        "            return parameters\n",
        "        try:\n",
        "            return parameters_to_ndarrays(parameters)\n",
        "        except Exception:\n",
        "\n",
        "            raise TypeError(f\"Unsupported parameters type: {type(parameters)}\")\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        # 1) Load global shared\n",
        "        arrs = self._normalize_incoming(parameters)\n",
        "        if arrs is not None:\n",
        "            keys = self.model.shared_keys()\n",
        "            self.model.load_shared_ndarrays(keys, arrs, strict=False)\n",
        "\n",
        "        # 2) FedProx snapshot (pre-update)\n",
        "        global_state = None\n",
        "        if config.get(\"strategy_name\", \"\") == \"FedProx\" and self.fedprox_mu > 0.0:\n",
        "            global_state = {k: v.detach().clone().cpu() for k, v in self.model.state_dict().items()}\n",
        "\n",
        "        # 3) Train for self.epochs, keep best-by-val AUROC\n",
        "        best_val = -float(\"inf\")\n",
        "        best_params_shared = None\n",
        "        best_metrics = None\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "\n",
        "            train_epoch(\n",
        "                self.model, self.dl_tr, self.opt, self.posw,\n",
        "                grad_clip=self.grad_clip,\n",
        "                fedprox_mu=(self.fedprox_mu if config.get(\"strategy_name\",\"\")==\"FedProx\" else 0.0),\n",
        "                global_state=global_state\n",
        "            )\n",
        "\n",
        "            yv, sv = predict_logits(self.model, self.dl_va)\n",
        "            aurocs, praucs = [], []\n",
        "            for h in self.horizons:\n",
        "                try:  aurocs.append(roc_auc_score(yv[h], sv[h]))\n",
        "                except: pass\n",
        "                try:  praucs.append(average_precision_score(yv[h], sv[h]))\n",
        "                except: pass\n",
        "            val_auroc_avg = float(np.mean(aurocs)) if aurocs else 0.0\n",
        "            val_prauc_avg = float(np.mean(praucs)) if praucs else 0.0\n",
        "\n",
        "            if val_auroc_avg > best_val:\n",
        "                best_val = val_auroc_avg\n",
        "                _, best_params_shared = self.model.get_shared_ndarrays()\n",
        "                best_metrics = {\n",
        "                    \"val_auroc_avg\": val_auroc_avg,\n",
        "                    \"val_prauc_avg\": val_prauc_avg,\n",
        "                    \"client_name\": self.name,\n",
        "                }\n",
        "\n",
        "\n",
        "        if best_params_shared is None:\n",
        "            _, best_params_shared = self.model.get_shared_ndarrays()\n",
        "            yv, sv = predict_logits(self.model, self.dl_va)\n",
        "            aurocs, praucs = [], []\n",
        "            for h in self.horizons:\n",
        "                try:  aurocs.append(roc_auc_score(yv[h], sv[h]))\n",
        "                except: pass\n",
        "                try:  praucs.append(average_precision_score(yv[h], sv[h]))\n",
        "                except: pass\n",
        "            best_metrics = {\n",
        "                \"val_auroc_avg\": float(np.mean(aurocs)) if aurocs else 0.0,\n",
        "                \"val_prauc_avg\": float(np.mean(praucs)) if praucs else 0.0,\n",
        "                \"client_name\": self.name,\n",
        "            }\n",
        "\n",
        "        # 4) Save full model on last round\n",
        "        rnd = int(config.get(\"round\", 1))\n",
        "        nrounds = int(config.get(\"num_rounds\", 1))\n",
        "        strategy_name = str(config.get(\"strategy_name\", \"FedAvg\"))\n",
        "        if rnd == nrounds:\n",
        "            save_dir = os.path.join(RESULTS_DIR, \"federated\", strategy_name, \"final_clients\")\n",
        "            pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(self.model.state_dict(), os.path.join(save_dir, f\"{self.name}_final.pt\"))\n",
        "\n",
        "        num_examples = len(self.dl_tr.dataset)\n",
        "        # NumPyClient: return (list[np.ndarray], int, dict)\n",
        "        return best_params_shared, num_examples, best_metrics\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        return 0.0, len(self.dl_te.dataset), {}\n"
      ],
      "metadata": {
        "id": "-rtEyvYpf6W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server strategies & federation orchestration\n",
        "Provides server-side strategies: FedAvg with round-wise weighted-metric logging and latest-weights retention, plus a β-Trimmed-Mean aggregator for robustness. Builds client/server apps with correctly initialized parameter ordering, runs the simulation, and saves per-round AUROC/PR-AUC plots and a round_log.json for analysis."
      ],
      "metadata": {
        "id": "fX0IhGGbVy96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell10\n",
        "class FedAvgKeepLast(FedAvg):\n",
        "    \"\"\"FedAvg that keeps latest aggregated parameters and logs round metrics.\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.latest_params = None\n",
        "        self.round_log = []  # list of dicts with aggregated fit metrics per round\n",
        "\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        out = super().aggregate_fit(rnd, results, failures)\n",
        "        if out is not None:\n",
        "            params, metrics_agg = out\n",
        "            if params is not None:\n",
        "               self.latest_params = parameters_to_ndarrays(params)\n",
        "\n",
        "        # aggregate client-returned metrics (weighted by examples)\n",
        "        if results:\n",
        "            total = sum([res.num_examples for _, res in results])\n",
        "            w_auroc = 0.0; w_prauc = 0.0\n",
        "            for _, res in results:\n",
        "                m = res.metrics or {}\n",
        "                n = res.num_examples\n",
        "                w_auroc += n * float(m.get(\"val_auroc_avg\", 0.0))\n",
        "                w_prauc += n * float(m.get(\"val_prauc_avg\", 0.0))\n",
        "            self.round_log.append({\n",
        "                \"round\": int(rnd),\n",
        "                \"val_auroc_avg_w\": float(w_auroc/total) if total>0 else 0.0,\n",
        "                \"val_prauc_avg_w\": float(w_prauc/total) if total>0 else 0.0\n",
        "            })\n",
        "        return out\n",
        "\n",
        "class TrimmedMeanStrategy(FedAvgKeepLast):\n",
        "    def __init__(self, beta=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.beta = beta\n",
        "    def aggregate_fit(self, rnd, results, failures):\n",
        "        if not results:\n",
        "            # record a placeholder so round indexing stays consistent\n",
        "            self.round_log.append({\n",
        "                \"round\": int(rnd),\n",
        "                \"val_auroc_avg_w\": 0.0,\n",
        "                \"val_prauc_avg_w\": 0.0\n",
        "            })\n",
        "            return None\n",
        "\n",
        "        # collect ndarrays from each client\n",
        "        all_nd = [parameters_to_ndarrays(res.parameters) for _, res in results]\n",
        "        # elementwise trimmed mean\n",
        "        agg = []\n",
        "        for layer_vals in zip(*all_nd):\n",
        "            stacked = np.stack(layer_vals, axis=0)\n",
        "            k = stacked.shape[0]\n",
        "            lo = int(np.floor(self.beta * k))\n",
        "            hi = int(np.ceil((1.0 - self.beta) * k))\n",
        "            trimmed = np.sort(stacked, axis=0)[lo:hi]\n",
        "            agg.append(trimmed.mean(axis=0))\n",
        "        # wrap and record\n",
        "        params = ndarrays_to_parameters(agg)\n",
        "        # log metrics same as parent:\n",
        "        total = sum([res.num_examples for _, res in results])\n",
        "        w_auroc = 0.0; w_prauc = 0.0\n",
        "        for _, res in results:\n",
        "            m = res.metrics or {}; n = res.num_examples\n",
        "            w_auroc += n * float(m.get(\"val_auroc_avg\", 0.0))\n",
        "            w_prauc += n * float(m.get(\"val_prauc_avg\", 0.0))\n",
        "        self.round_log.append({\n",
        "            \"round\": int(rnd),\n",
        "            \"val_auroc_avg_w\": float(w_auroc/total) if total>0 else 0.0,\n",
        "            \"val_prauc_avg_w\": float(w_prauc/total) if total>0 else 0.0\n",
        "        })\n",
        "        self.latest_params = [a.copy() for a in agg]\n",
        "        return (params, {})  # (parameters, metrics)\n",
        "\n",
        "def build_client_app():\n",
        "    def client_fn(context: Context) -> Client:\n",
        "        pid = int(context.node_config[\"partition-id\"])\n",
        "        name = PID_TO_NAME[pid]\n",
        "        return IoMTClient(name).to_client()\n",
        "    return ClientApp(client_fn=client_fn)\n",
        "\n",
        "def build_server_app(strategy_name=\"FedAvg\", trimmed_beta=0.1):\n",
        "    # initialize global params using SAME ordering as client shares (fix a)\n",
        "    any_dir = CLIENTS[CLIENT_NAMES[0]]\n",
        "    _, _, _, Fdim = make_loaders(any_dir, HORIZONS, BATCH)\n",
        "    init = HIPFedPredict(Fdim, HORIZONS).to(device)\n",
        "    _, arrs = init.get_shared_ndarrays()\n",
        "    initial_parameters = ndarrays_to_parameters(arrs)\n",
        "\n",
        "    if strategy_name == \"FedAvg\":\n",
        "        strategy = FedAvgKeepLast(\n",
        "            fraction_fit=1.0, fraction_evaluate=0.0,\n",
        "            min_fit_clients=FED_NUM_CLIENTS, min_available_clients=FED_NUM_CLIENTS,\n",
        "            initial_parameters=initial_parameters,\n",
        "            on_fit_config_fn=lambda r: {\"round\": r, \"num_rounds\": FED_NUM_ROUNDS,\n",
        "                                        \"local_epochs\": FED_EPOCHS_PER_ROUND,\n",
        "                                        \"strategy_name\": \"FedAvg\"},\n",
        "        )\n",
        "    elif strategy_name == \"TrimmedMean\":\n",
        "        strategy = TrimmedMeanStrategy(\n",
        "            beta=trimmed_beta,\n",
        "            fraction_fit=1.0, fraction_evaluate=0.0,\n",
        "            min_fit_clients=FED_NUM_CLIENTS, min_available_clients=FED_NUM_CLIENTS,\n",
        "            initial_parameters=initial_parameters,\n",
        "            on_fit_config_fn=lambda r: {\"round\": r, \"num_rounds\": FED_NUM_ROUNDS,\n",
        "                                        \"local_epochs\": FED_EPOCHS_PER_ROUND,\n",
        "                                        \"strategy_name\": \"TrimmedMean\"},\n",
        "        )\n",
        "    elif strategy_name == \"FedProx\":\n",
        "        # aggregation is FedAvg; proximal is applied on clients (fix b)\n",
        "        strategy = FedAvgKeepLast(\n",
        "            fraction_fit=1.0, fraction_evaluate=0.0,\n",
        "            min_fit_clients=FED_NUM_CLIENTS, min_available_clients=FED_NUM_CLIENTS,\n",
        "            initial_parameters=initial_parameters,\n",
        "            on_fit_config_fn=lambda r: {\"round\": r, \"num_rounds\": FED_NUM_ROUNDS,\n",
        "                                        \"local_epochs\": FED_EPOCHS_PER_ROUND,\n",
        "                                        \"strategy_name\": \"FedProx\"},\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unknown strategy\")\n",
        "\n",
        "    def server_fn(_: Context) -> ServerAppComponents:\n",
        "        return ServerAppComponents(strategy=strategy, config=ServerConfig(num_rounds=FED_NUM_ROUNDS))\n",
        "\n",
        "    return ServerApp(server_fn=server_fn), strategy\n",
        "\n",
        "def run_federated(strategy_name=\"FedAvg\", trimmed_beta=FED_TRIMMED_BETA):\n",
        "    client_app = build_client_app()\n",
        "    server_app, strategy = build_server_app(strategy_name=strategy_name, trimmed_beta=trimmed_beta)\n",
        "\n",
        "    # run\n",
        "    run_simulation(\n",
        "        server_app=server_app,\n",
        "        client_app=client_app,\n",
        "        num_supernodes=FED_NUM_CLIENTS,\n",
        "        backend_config={\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1}},\n",
        "    )\n",
        "\n",
        "    # save round curves\n",
        "    rounds = [r[\"round\"] for r in strategy.round_log]\n",
        "    va = [r[\"val_auroc_avg_w\"] for r in strategy.round_log]\n",
        "    vp = [r[\"val_prauc_avg_w\"] for r in strategy.round_log]\n",
        "    plt.figure(); plt.plot(rounds, va, marker=\"o\"); plt.xlabel(\"Round\"); plt.ylabel(\"Val AUROC (avg)\")\n",
        "    savefig(os.path.join(RESULTS_DIR, \"federated\", strategy_name, \"val_auroc_rounds.png\"))\n",
        "    plt.figure(); plt.plot(rounds, vp, marker=\"o\"); plt.xlabel(\"Round\"); plt.ylabel(\"Val PR-AUC (avg)\")\n",
        "    savefig(os.path.join(RESULTS_DIR, \"federated\", strategy_name, \"val_prauc_rounds.png\"))\n",
        "\n",
        "    # save history json\n",
        "    save_json(strategy.round_log, os.path.join(RESULTS_DIR, \"federated\", strategy_name, \"round_log.json\"))\n",
        "    return strategy\n"
      ],
      "metadata": {
        "id": "74E94ngpf9Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final federated evaluation (per client)**\n",
        "Loads each client’s saved final model, overwrites only the shared trunk with the server’s latest global weights, re-selects thresholds on that client’s validation set, evaluates on its test set, and saves calibration plots and a consolidated final_reports.json under results/federated/<strategy>/."
      ],
      "metadata": {
        "id": "QSll7Ft-V8zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell11\n",
        "@torch.no_grad()\n",
        "def eval_federated_final(strategy, strategy_name=\"FedAvg\"):\n",
        "    # latest global shared (trunk/proj, no BN, no heads)\n",
        "    global_shared = strategy.latest_params\n",
        "    reports = {}\n",
        "\n",
        "    for cname, cdir in CLIENTS.items():\n",
        "        # Rebuild model with the right input dim\n",
        "        _, _, _, F = make_loaders(cdir, HORIZONS, BATCH)\n",
        "        model = HIPFedPredict(F, HORIZONS).to(device)\n",
        "\n",
        "        # Load the saved client model (has private heads and local BN)\n",
        "        ckpt_path = os.path.join(RESULTS_DIR, \"federated\", strategy_name, \"final_clients\", f\"{cname}_final.pt\")\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            print(f\"[WARN] Missing saved model for {cname}: {ckpt_path}\")\n",
        "            continue\n",
        "        state = torch.load(ckpt_path, map_location=device)\n",
        "        model.load_state_dict(state, strict=False)\n",
        "\n",
        "        # Overwrite only the shared (global) parts\n",
        "        keys = model.shared_keys()\n",
        "        model.load_shared_ndarrays(keys, global_shared, strict=False)\n",
        "\n",
        "        # Evaluate: thresholds from val, then test\n",
        "        dl_tr, dl_va, dl_te, _ = make_loaders(cdir, HORIZONS, BATCH)\n",
        "        yv, sv = predict_logits(model, dl_va)\n",
        "        val_thr = {h: pick_threshold_from_val(yv[h], sv[h]) for h in HORIZONS}\n",
        "        yt, st = predict_logits(model, dl_te)\n",
        "        _, _, Kte = load_split_arrays_with_k(cdir, \"test\", HORIZONS)\n",
        "        rep = pack_metrics_per_horizon(HORIZONS, yt, st, val_thr, k_test=Kte)\n",
        "\n",
        "        # Calibration figs\n",
        "        for h in HORIZONS:\n",
        "            plt.figure()\n",
        "            plot_reliability(plt.gca(), yt[h], st[h], title=f\"{strategy_name}:{cname} (h={h})\")\n",
        "            savefig(os.path.join(RESULTS_DIR, \"federated\", strategy_name, \"calibration\", f\"{cname}_h{h}.png\"))\n",
        "\n",
        "        reports[cname] = rep\n",
        "\n",
        "    save_json(reports, os.path.join(RESULTS_DIR, \"federated\", strategy_name, \"final_reports.json\"))\n",
        "    print(f\"[{strategy_name}] final per-client reports saved.\")\n",
        "    return reports\n"
      ],
      "metadata": {
        "id": "UVqdPXjegAIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final toggles — Select and run experiments**\n",
        "Simple flags choose which pipelines to execute (local-only, centralized oracle, and federated variants: FedAvg, Trimmed-Mean, FedProx). Running this cell executes the selected experiments, performs training/evaluation, and prints where outputs are stored."
      ],
      "metadata": {
        "id": "_-Yb3ZNUV_KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Toggles\n",
        "RUN_LOCAL_ONLY   = True\n",
        "RUN_CENTRALIZED  = False\n",
        "RUN_FEDAVG       = False\n",
        "RUN_TRIMMED_MEAN = False\n",
        "RUN_FEDPROX      = False\n",
        "\n",
        "# 1) Local-only baselines (per client)\n",
        "if RUN_LOCAL_ONLY:\n",
        "    print(\"\\n=== Running Local-only baselines ===\")\n",
        "    run_local_only_all()\n",
        "\n",
        "# 2) Centralized (oracle)\n",
        "if RUN_CENTRALIZED:\n",
        "    print(\"\\n=== Running Centralized (oracle on ALIGNED features) ===\")\n",
        "    run_centralized_oracle()\n",
        "\n",
        "# 3) Federated (simulation)\n",
        "\n",
        "if RUN_FEDAVG:\n",
        "    print(\"\\n=== Federated: FedAvg ===\")\n",
        "    strat_fa = run_federated(strategy_name=\"FedAvg\")\n",
        "    reports_fa = eval_federated_final(strat_fa, \"FedAvg\")\n",
        "\n",
        "if RUN_TRIMMED_MEAN:\n",
        "    print(\"\\n=== Federated: Trimmed-Mean ===\")\n",
        "    strat_tm = run_federated(strategy_name=\"TrimmedMean\", trimmed_beta=FED_TRIMMED_BETA)\n",
        "    reports_tm = eval_federated_final(strat_tm, \"TrimmedMean\")\n",
        "\n",
        "if RUN_FEDPROX:\n",
        "    print(\"\\n=== Federated: FedProx ===\")\n",
        "\n",
        "    strat_fp = run_federated(strategy_name=\"FedProx\")\n",
        "    reports_fp = eval_federated_final(strat_fp, \"FedProx\")\n",
        "\n",
        "print(\"\\nAll selected runs finished. Outputs live under:\", RESULTS_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRENVpBWgPXN",
        "outputId": "d3e3d509-ea10-4ee4-938e-628e9e5f5e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running Local-only baselines ===\n",
            "[LOCAL] ICE\n",
            "[HIPFedPredict] ep 01/20 | train_loss=0.2818 | val_AUROC=0.8788 | val_PRAUC=0.8871 | best_ep=1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 02/20 | train_loss=0.2524 | val_AUROC=0.8791 | val_PRAUC=0.8871 | best_ep=2 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 03/20 | train_loss=0.2469 | val_AUROC=0.8844 | val_PRAUC=0.8936 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 04/20 | train_loss=0.2431 | val_AUROC=0.8797 | val_PRAUC=0.8913 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 05/20 | train_loss=0.2384 | val_AUROC=0.8815 | val_PRAUC=0.8935 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 06/20 | train_loss=0.2342 | val_AUROC=0.8795 | val_PRAUC=0.8911 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 07/20 | train_loss=0.2316 | val_AUROC=0.8860 | val_PRAUC=0.8945 | best_ep=7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 08/20 | train_loss=0.2283 | val_AUROC=0.8777 | val_PRAUC=0.8852 | best_ep=7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 09/20 | train_loss=0.2258 | val_AUROC=0.8707 | val_PRAUC=0.8809 | best_ep=7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 10/20 | train_loss=0.2238 | val_AUROC=0.8698 | val_PRAUC=0.8757 | best_ep=7 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 11/20 | train_loss=0.2205 | val_AUROC=0.8749 | val_PRAUC=0.8857 | best_ep=7 \n",
            "[HIPFedPredict] Early stop at epoch 11 (patience=4, best_ep=7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOCAL] IOMT_A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 01/20 | train_loss=0.0028 | val_AUROC=0.9986 | val_PRAUC=0.9988 | best_ep=1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 02/20 | train_loss=0.0016 | val_AUROC=0.9984 | val_PRAUC=0.9986 | best_ep=1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 03/20 | train_loss=0.0014 | val_AUROC=0.9999 | val_PRAUC=0.9999 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 04/20 | train_loss=0.0014 | val_AUROC=0.9999 | val_PRAUC=0.9999 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 05/20 | train_loss=0.0014 | val_AUROC=0.9998 | val_PRAUC=0.9999 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 06/20 | train_loss=0.0013 | val_AUROC=0.9999 | val_PRAUC=0.9999 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 07/20 | train_loss=0.0012 | val_AUROC=0.9999 | val_PRAUC=0.9999 | best_ep=3 \n",
            "[HIPFedPredict] Early stop at epoch 7 (patience=4, best_ep=3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOCAL] IOMT_B\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 01/20 | train_loss=0.0010 | val_AUROC=0.9997 | val_PRAUC=0.9998 | best_ep=1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 02/20 | train_loss=0.0004 | val_AUROC=0.9998 | val_PRAUC=0.9999 | best_ep=1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 03/20 | train_loss=0.0003 | val_AUROC=0.9999 | val_PRAUC=0.9999 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 04/20 | train_loss=0.0003 | val_AUROC=0.9999 | val_PRAUC=0.9999 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 05/20 | train_loss=0.0004 | val_AUROC=0.9998 | val_PRAUC=0.9999 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 06/20 | train_loss=0.0003 | val_AUROC=0.9995 | val_PRAUC=0.9997 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 07/20 | train_loss=0.0003 | val_AUROC=0.9998 | val_PRAUC=0.9999 | best_ep=3 \n",
            "[HIPFedPredict] Early stop at epoch 7 (patience=4, best_ep=3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOCAL] WUSTL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 01/20 | train_loss=2.4745 | val_AUROC=0.9478 | val_PRAUC=0.6812 | best_ep=1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 02/20 | train_loss=1.8824 | val_AUROC=0.9374 | val_PRAUC=0.6940 | best_ep=1 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 03/20 | train_loss=1.7392 | val_AUROC=0.9571 | val_PRAUC=0.7612 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 04/20 | train_loss=1.6582 | val_AUROC=0.9292 | val_PRAUC=0.7014 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 05/20 | train_loss=1.4911 | val_AUROC=0.9459 | val_PRAUC=0.7710 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 06/20 | train_loss=1.3226 | val_AUROC=0.9197 | val_PRAUC=0.6605 | best_ep=3 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HIPFedPredict] ep 07/20 | train_loss=1.1672 | val_AUROC=0.9299 | val_PRAUC=0.7145 | best_ep=3 \n",
            "[HIPFedPredict] Early stop at epoch 7 (patience=4, best_ep=3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOCAL] done.\n",
            "\n",
            "All selected runs finished. Outputs live under: /content/drive/MyDrive/fed_MID/results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    }
  ]
}